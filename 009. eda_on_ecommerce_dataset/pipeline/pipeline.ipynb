{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on Vehicle Sales Data\n",
    "My client company is **--**, and they have give me a dataset of vehicle sales data. As a data scientist they want me to conduct Data Cleaning, Descriptive Statististics, Data Visualization and ferential Statistics.\n",
    " In this notebook I will be conducting and Exploratory Data Analysis (EDA) on Vehicle Sales Data dataset and try to get an insights of the dataset. By doing this I can find the answers to various sales questions, such as, top selling cars,**ADD HERE**.I will be loading the dataset and then try to find and any relationships, treands and patterns between among the various features of the dataset. Also, I will be trying to find answers to the below set of questions.\n",
    "\n",
    "Questions\n",
    "1. What is the trend in vehicle sales over the years based on the \"year\" column?\n",
    "2. Which vehicle make dominates the sales in the dataset?\n",
    "3. Are there any particular models that consistently perform better in terms of sales?\n",
    "4. How does the distribution of vehicle trims affect their selling prices?\n",
    "5. What body type is most commonly sold in the dataset?\n",
    "6. Is there any correlation between transmission type and selling price?\n",
    "7. Does the VIN (Vehicle Identification Number) have any influence on sales performance?\n",
    "8. Which states contribute the most to overall vehicle sales?\n",
    "9. How does the condition of the vehicle impact its selling price?\n",
    "10. Is there a relationship between odometer reading and selling price?\n",
    "11. Are certain colors more popular among buyers, and do they affect selling prices?\n",
    "12. Does the interior type influence the selling price of a vehicle?\n",
    "13. What type of sellers (e.g., dealerships, private sellers) are more successful in selling vehicles?\n",
    "14. Is there a correlation between the Manheim Market Report (MMR) value and the actual selling price of vehicles?\n",
    "15. How does the sale date (month, day of the week) affect the selling price and volume of vehicle sales?\n",
    "\n",
    "**About the data**  \n",
    "This [**dataset**](https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data/code) contains 16 columns of sales data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing the necessary libraries for my Exploratory Data Analysis tasks in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/009. eda_on_ecommerce_dataset/dataset/car_prices.csv')\n",
    "\n",
    "print(\"Number of rows.   :\", df.shape[0])\n",
    "print(\"Number of columns.   :\", df.shape[1])\n",
    "print(\"=\"*130)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Data Dimensions\n",
    "\n",
    "After loading the dataset into the DataFrame `df`, I'm interested in knowing its dimensions of the datafame.\n",
    "By Using the `.shape` attribute , I will retrieve the number of rows and columns in the form of a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above there are 5558837 rows and 16 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a short summary of the dataframe\n",
    "\n",
    "- Here we use`info()` method to get a shortsummary of the DataFrame. It will include the number of non-null values, Data Types, and Memory Usage etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above response the count in the **Non-Null** column is less than the total columns i.e. 558837, that means that there are many NULL values in the dataset. Which we will take care in the data cleaning part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us check datatypes of the columns\n",
    "- I will be using `dtypes` attribute to get the datatypes of the columns.\n",
    "- If you have notices we also got the same details in `.info()` attribute above. But I like to view it seperately we get a getter understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a Descriptive Statistics\n",
    "\n",
    "- `df.describe()`will help us generate a descriptive statistics of numerical columns of the DataFrame. It will provide us various functions such as mean, count, Standard Deviation, Min, Max and percentiles.\n",
    "- Let's check and try to study our dataset.\n",
    "- I have used Transpose here as I want the columns of my DataFrame as the index. This especially helps when I have many numrical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HAS TO BE RE-WRITTEN**\n",
    "1. **Year**:\n",
    "   - The dataset spans from the year 1982 to 2015, with the majority of vehicles being sold between 2007 and 2013.\n",
    "   - The average year of sale is approximately 2010, indicating that the dataset predominantly consists of relatively recent vehicle sales.\n",
    "\n",
    "2. **Condition**:\n",
    "   - The condition values range from 1 to 49, with higher values representing better conditions.\n",
    "   - The average condition value is around 30, suggesting that the vehicles in the dataset generally have moderate to good conditions.\n",
    "\n",
    "3. **Odometer**:\n",
    "   - Odometer readings range from 1 to 999,999 miles.\n",
    "   - The average odometer reading is approximately 68,320 miles, indicating that the vehicles vary widely in terms of mileage.\n",
    "   - The median (50th percentile) odometer reading is 52,254 miles, suggesting that half of the vehicles have odometer readings below this value.\n",
    "\n",
    "4. **MMR (Market Median Retail)**:\n",
    "   - MMR values represent the median retail price of vehicles in the market.\n",
    "   - The MMR values range from 25 to 182,000, with an average MMR of approximately 13,769.\n",
    "   - The median MMR is 12,250, indicating that half of the vehicles in the dataset have market median retail prices below this value.\n",
    "\n",
    "5. **Selling Price**:\n",
    "   - Selling prices of vehicles in the dataset range from 1 to 230,000.\n",
    "   - The average selling price is approximately 13,611, suggesting that the dataset contains a mix of vehicles across different price ranges.\n",
    "   - The median selling price is 12,100, indicating that half of the vehicles were sold at prices below this value.\n",
    "\n",
    "Overall, this summary provides valuable insights into the distribution and characteristics of the vehicle sales dataset, including information about the years of sale, vehicle conditions, mileage, market median retail prices, and selling prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows.   :\", df.shape[0])\n",
    "print(\"=\"*30)\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to check how much percentage of the columns are null. If they are negligible percentage then we can just drop the columns. Otherwise I will fill the na values with the meaninfull values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentage = {}\n",
    "for column in df:\n",
    "    null_percentage[column] = round(( ( df[column].isnull().sum() ) / len(df)) * 100,2)\n",
    "np_df = pd.DataFrame( list( null_percentage.items() ),  columns=[  'column_name'  , 'null_percentage' ])\n",
    "np_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"state\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"transmission\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the null_percentage for each column is negligible. The max is for Transmission is 11.69%, which is negligible when we consider the datasize, i.e. 558837 of data. Hence I will be droping the rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows.   :\", df.shape[0])\n",
    "print(\"=\"*30)\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIN number is not usefull for my analysis, hence I will be droping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"vin\",axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can also see that sale date is different format. So I am going to use `dateutil.parser` module to clean it and convert it to datetime format. This module will automatically detect varies formats and convert it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['saledatetime']  =    pd.to_datetime(df['saledate' ].apply( parser.parse),utc=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can \n",
    "1. drop the older  `saledate` column, \n",
    "2. create a new `saledate` column which will have only the saledate and remove the time as it is not important for us.\n",
    "3. drop the `saledatetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"saledate\",axis=1,inplace=True)\n",
    "df[\"saledate\"] = df[\"saledatetime\"].dt.date\n",
    "df.drop(\"saledatetime\",axis=1,inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also want to rearrange the columns in the dataframe for better visiblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearranged_columns = ['year','make','model','trim','body','transmission','state','condition','color','interior','saledate','seller','sellingprice','mmr','odometer']\n",
    "df = df[rearranged_columns]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df['saledate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df[df[['make', 'model', 'trim', 'body', 'transmission']].isnull().any(axis=1)]\n",
    "\n",
    "\n",
    "df[df[['make','model', 'trim', 'body', 'transmission']].isnull().any(axis=1)]\n",
    "\n",
    "\n",
    "# I can see that only automatic transmission is fill. I will check if manual is filled. Otherwise I am going to fill it will word \"Manual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"transmission\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = df[df[\"transmission\"] == 'sedan']\n",
    "selected_rows\n",
    "\n",
    "# I can see that for the same make, model, trim and body the transmission is sedan, whereas it should have been automatic. I will change it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[(df['make'] == 'Volkswagen') & \n",
    "                (df['model'] == 'Jetta') & \n",
    "                (df['trim'] == 'SE PZEV w/Connectivity') & \n",
    "                (df['body'] == 'Navitgation')]\n",
    "\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df.loc[(df['make'] == 'Volkswagen') & \n",
    "       (df['model'] == 'Jetta') & \n",
    "       (df['trim'] == 'SE PZEV w/Connectivity\t') & \n",
    "       (df['body'] == 'Navitgation'), 'transmission'] = 'automatic'\n",
    "\n",
    "selected_rows = df[df[\"transmission\"] == 'sedan']\n",
    "selected_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "# For the image quality of the graphic. \n",
    "sns.set_theme(rc={\"figure.dpi\":400})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the size of the graphics\n",
    "sns.set_theme(rc={\"figure.figsize\":(6,3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data for Training and Testing\n",
    "\n",
    "Now we have reached a crucial step: splitting the data into training and testing sets. This ensures that I can train my model on one subset of the data and evaluate its performance on another, unseen subset.\n",
    "\n",
    "Using the `train_test_split` function from the `sklearn.model_selection` module, I split my dataset into training and testing sets. Specifically, I split the features (`df['year']`) and the target variable (`df['per capita income (US$)']`) into training and testing sets, with a test size of 0.25 (25%).\n",
    "\n",
    "The resulting sets are:\n",
    "\n",
    "- `X_train`: The training features, representing years.\n",
    "- `X_test`: The testing features, also representing years.\n",
    "- `y_train`: The target variable for training, representing per capita income (US$).\n",
    "- `y_test`: The target variable for testing, also representing per capita income (US$).\n",
    "\n",
    "After the split, I inspect the shapes of these sets to ensure they align correctly. The shapes are as follows:\n",
    "\n",
    "- Shape of X_train: (shape of the training feature set)\n",
    "- Shape of X_test: (shape of the testing feature set)\n",
    "- Shape of y_train: (shape of the training target set)\n",
    "- Shape of y_test: (shape of the testing target set)\n",
    "\n",
    "These shapes provide insights into the distribution of data between training and testing sets, ensuring that my model receives a balanced and representative subset for both training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(df.year,df['per capita income (US$)'], test_size=0.25)\n",
    "\n",
    "print(f\"Shape of X_train is: {X_train.shape}\")\n",
    "print(f\"Shape of X_test is: {X_test.shape}\")\n",
    "print(f\"Shape of y_train is: {y_train.shape}\")\n",
    "print(f\"Shape of y_train is: {y_test.shape}\")\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am skipping the Feature Engineering step as I do not see any scope for transforming our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = reg.predict(X_test)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot and check how close is it to our actuall data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"PCI in US\")\n",
    "plt.scatter(X_test,y_test,color='black')\n",
    "plt.plot(X_test,y_predicted,color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.ravel()\n",
    "y_test = y_test.ravel()\n",
    "y_predicted = y_predicted.ravel()\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"PCI in US\")\n",
    "sns.scatterplot(x=X_test,y=y_test,color='black')\n",
    "sns.lineplot(x=X_test,y=y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen the predition line and the scatter plots. We can measure the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(-1, 1)\n",
    "# X_test = X_test.reshape(-1, 1)\n",
    "# y_train = y_train.reshape(-1, 1)\n",
    "# y_test = y_test.reshape(-1, 1)\n",
    "# y_predicted = y_predicted.reshape(-1,1)\n",
    "\n",
    "MAE = mean_absolute_error(y_test,y_predicted)\n",
    "MSE = mean_squared_error(y_test,y_predicted)\n",
    "R2 = r2_score(y_test,y_predicted)\n",
    "\n",
    "print(f\"MAE is {MAE}\")\n",
    "print(f\"MSE is {MSE}\")\n",
    "print(f\"R2  is {R2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
