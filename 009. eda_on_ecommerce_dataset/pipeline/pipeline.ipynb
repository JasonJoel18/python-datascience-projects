{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "We will build a machine learning pipeline using a linear regression model. In particular, you should do the following:\n",
    "Steps:-\n",
    "- Load the `canada_per_capita_income` dataset using [Pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). You can find this dataset in the datasets folder.\n",
    "- Split the dataset into training and test sets using [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). \n",
    "- Conduct data exploration, data preprocessing, and feature engineering if necessary. \n",
    "- Train and test a linear regression model using [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "- Check the documentation to identify the most important hyperparameters, attributes, and methods of the model. Use them in practice.\n",
    "- Give a Conclusion to the of the study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing the necessary libraries for my data analysis and machine learning tasks in Python. The `pandas` library helps me handle data efficiently, so I import it as `pd`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from   sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from   sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "# This is a test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data for Per Capita Income Prediction\n",
    "\n",
    "Then, I read a CSV file containing data on Canada's per capita income over the years.\\\n",
    "- The file path is specified as `.../canada_per_capita_income.csv`.\n",
    "- Using `pd.read_csv()`, I load the data from the CSV file into a pandas DataFrame named `df`.\\\n",
    "- This DataFrame serves as the primary data structure for my analysis.\n",
    "- Once the data is loaded, I can explore its structure, characteristics, and start preparing it for further analysis or machine learning model development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/008. predicting_per_capita_income_based_on_year/dataset/canada_per_capita_income.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Data Dimensions\n",
    "\n",
    "After loading the dataset into the DataFrame `df`, I'm interested in understanding its dimensions, which helps me grasp the size of the dataset.\n",
    "\n",
    "Using the `.shape` attribute of the DataFrame, I retrieve a tuple containing two values: the number of rows and the number of columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "`df.info()`:\n",
    "- This line calls the `info()` method on the DataFrame `df`.\n",
    "- The `info()` method provides a concise summary of the DataFrame, including the number of non-null values, data types, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "`df.dtypes`:\n",
    "- This line retrieves the data types of each column in the DataFrame `df`.\n",
    "- The `dtypes` attribute in pandas DataFrame provides information about the data types of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "`df.describe()`:\n",
    "- This line generates descriptive statistics summarizing the central tendency, dispersion, and shape of the numerical columns in the DataFrame `df`.\n",
    "- The `describe()` function provides statistics such as count, mean, standard deviation, minimum, maximum, and quartile values for each numerical column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "# You can transpose this using this code `df.describe().T`, if you have more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x=\"per capita income (US$)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "# For the image quality of the graphic. \n",
    "sns.set_theme(rc={\"figure.dpi\":400})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the size of the graphics\n",
    "sns.set_theme(rc={\"figure.figsize\":(6,3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data for Training and Testing\n",
    "\n",
    "Now we have reached a crucial step: splitting the data into training and testing sets. This ensures that I can train my model on one subset of the data and evaluate its performance on another, unseen subset.\n",
    "\n",
    "Using the `train_test_split` function from the `sklearn.model_selection` module, I split my dataset into training and testing sets. Specifically, I split the features (`df['year']`) and the target variable (`df['per capita income (US$)']`) into training and testing sets, with a test size of 0.25 (25%).\n",
    "\n",
    "The resulting sets are:\n",
    "\n",
    "- `X_train`: The training features, representing years.\n",
    "- `X_test`: The testing features, also representing years.\n",
    "- `y_train`: The target variable for training, representing per capita income (US$).\n",
    "- `y_test`: The target variable for testing, also representing per capita income (US$).\n",
    "\n",
    "After the split, I inspect the shapes of these sets to ensure they align correctly. The shapes are as follows:\n",
    "\n",
    "- Shape of X_train: (shape of the training feature set)\n",
    "- Shape of X_test: (shape of the testing feature set)\n",
    "- Shape of y_train: (shape of the training target set)\n",
    "- Shape of y_test: (shape of the testing target set)\n",
    "\n",
    "These shapes provide insights into the distribution of data between training and testing sets, ensuring that my model receives a balanced and representative subset for both training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(df.year,df['per capita income (US$)'], test_size=0.25)\n",
    "\n",
    "print(f\"Shape of X_train is: {X_train.shape}\")\n",
    "print(f\"Shape of X_test is: {X_test.shape}\")\n",
    "print(f\"Shape of y_train is: {y_train.shape}\")\n",
    "print(f\"Shape of y_train is: {y_test.shape}\")\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am skipping the Feature Engineering step as I do not see any scope for transforming our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = reg.predict(X_test)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot and check how close is it to our actuall data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"PCI in US\")\n",
    "plt.scatter(X_test,y_test,color='black')\n",
    "plt.plot(X_test,y_predicted,color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.ravel()\n",
    "y_test = y_test.ravel()\n",
    "y_predicted = y_predicted.ravel()\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"PCI in US\")\n",
    "sns.scatterplot(x=X_test,y=y_test,color='black')\n",
    "sns.lineplot(x=X_test,y=y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen the predition line and the scatter plots. We can measure the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.reshape(-1, 1)\n",
    "# X_test = X_test.reshape(-1, 1)\n",
    "# y_train = y_train.reshape(-1, 1)\n",
    "# y_test = y_test.reshape(-1, 1)\n",
    "# y_predicted = y_predicted.reshape(-1,1)\n",
    "\n",
    "MAE = mean_absolute_error(y_test,y_predicted)\n",
    "MSE = mean_squared_error(y_test,y_predicted)\n",
    "R2 = r2_score(y_test,y_predicted)\n",
    "\n",
    "print(f\"MAE is {MAE}\")\n",
    "print(f\"MSE is {MSE}\")\n",
    "print(f\"R2  is {R2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
