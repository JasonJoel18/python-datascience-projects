{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1 style=\"text-align: center;\">Image Classification  Project</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions that I have\n",
    "1. Shall I delete the images that are having the error sRGB first and then scale or visa versa\n",
    "2. How to tune the machine learning model.\n",
    "3. Config 8 was the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëãüèº **Introduction**\n",
    "\n",
    "<p style=\"text-align: justify;\">I am a data scientist for a company called Secure Cams. And it is building an application in which if you take pictures of gym equipments, it should be able to detect which equipment it is. The problem that currently the company is facing it there are many equipemnts in the gym which people have paid subscription for but they are not using it because they do not know what the equipment is and they do not know how ot use it. So, with this project the aim of my company is to build a deep leaning pipeline and implement this in their gym application. So that an user can take a picture of an equipment and will be able to find out what the equipment is and how to use it.</p>\n",
    "\n",
    "## üßø **Objective**\n",
    " <p style=\"text-align: justify;\">The objective of this project is that I will have to build a module which will try to classify the category of gym equipment. For this my company has given me a dataset which has images of many equipments which are divided into diffrent folders. So I will be using these images in the folders and building an accurate Deep Learning Model using Convolutional Neural Network. Firstly I will be going through the full image dataset and trying to remove the unwanted images with unsupported extensions. And then using these images I will be building a model. I will also be doing several experiments in order to improve the performance of the model. I will be including these test results in the I will check if our dataset is balanced and if there is a requirement to increase the dataset so that our model can learn better from the data. Then I will choose the best-performing model to build my final model with the best-suited parameters.\n",
    " </p>\n",
    "\n",
    "**About the data**  \n",
    "This dataset contains 12 columns. This data set is from Kaggle. You can [**click here**](https://www.kaggle.com/datasets/starsiwach/4-gym-equipment-types-classification-dataset) to view the dataset on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Content**\n",
    "\n",
    "##### ‚¨á [**Importing Libraries**](#library)\n",
    "##### üìä [**Importing Dataset**](#dataset)\n",
    "##### üó∫Ô∏è [**Exploring the dataset**](#exploration)\n",
    "##### üí≠ [**My taughts on the dataset**](#thoughts)\n",
    "##### ‚öôÔ∏è [**Pre-processing the data**](#processing)\n",
    "##### üîß [**Feature Engineering**](#feature)\n",
    "##### üóÇÔ∏è [**Selecting the best model for our dataset**](#selection)\n",
    "##### ‚úÖ [**Assessing which gives us the best results**](#assess)\n",
    "##### üéõÔ∏è [**Hyper-Parameter Tuning**](#tuning)\n",
    "##### üèÅ [**Final Model**](#Final)\n",
    "##### üí° [**Conclusion**](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Let's begin....***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='library'></a>\n",
    "## ‚¨á <span style=\"color: #20479b; font-weight: bold;\">Importing Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing the necessary libraries for my Exploratory Data Analysis tasks in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as  pd\n",
    "import os\n",
    "import warnings\n",
    "import    numpy as  np\n",
    "import cv2\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import imghdr\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"cv2\")\n",
    "import plotly.express as px\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, MaxPooling2D, Conv2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>  \n",
    "\n",
    "## üìä <span style=\"color: #20479b; font-weight: bold;\">Importing Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst_dir = '/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/014. image_classification_project/dataset/Gym Data'\n",
    "extensions = ['png','jpeg','jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have specified the data directory where I have saved my images in my local computer.\n",
    "I have also created a list of extensions which I will be taking into consideration in this pipeline. I will be deleting everything else.\n",
    "Let's check how many folders/categories are there in my directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(dtst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ignore the DS store which is the defaut masos folder. We can see that there are 4 folders. Which means that we have four types of equiments which we need to build a machine learning model from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(dtst_dir,'Recumbent Bike'))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am selecting first 10 image names in the recumbent Bike folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have built the below code so that it can run though all the images and remove the images which do not support the extensions list that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extensions)\n",
    "from PIL import Image\n",
    "for folder in os.listdir(dtst_dir):\n",
    "    if folder == '.DS_Store':\n",
    "        continue\n",
    "    folder_dir = os.path.join(dtst_dir, folder)\n",
    "    for file in  os.listdir(folder_dir):\n",
    "        file_dir = os.path.join(folder_dir, file) \n",
    "        try:\n",
    "            extension = imghdr.what(file_dir)\n",
    "            # print(extension)\n",
    "            # print(file_dir)\n",
    "            # if extension == 'png':\n",
    "            #     with Image.open(file_path) as img:\n",
    "            #         img.save(file_path)\n",
    "                    \n",
    "            image = cv2.imread(file_dir)\n",
    "            \n",
    "            if extension not in extensions:\n",
    "                print(f'Extension {extension} not supported, hence will be deleted >> {file_dir}')\n",
    "                os.remove(file_dir)\n",
    "        except Exception as exp:\n",
    "            print(f'Extension error for {extension} >> {file_dir}')\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module='cv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there an image is saved in array. It will be in a matrix format. Let's check the shape of image and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here it has three values. the first 156 represents the height of the image. Second number represents the width of the image. And the third one is the color. Since we have a colored images it will be stored in 3 channels that is red green blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the type of the datatype for the variable image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tf.keras.utils.image_dataset_from_directory(dtst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## üó∫Ô∏è <span style=\"color: #20479b; font-weight: bold;\">Exploring the dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "For EDA I will be using the main data. Once data is normalized, I will be splitting it into `train` and `test` split.\n",
    "1. Screening the images in the pipeline one by one takes a lot of time. Hence we will be looking at only few of the images and making sure that it correctly maintained.\n",
    "2. I will be splitting the data after normalization because otherwise, the three datasets won't be even. \n",
    "\n",
    "Here one disadvantage that I saw is that we cannot view the image randomly. We need a iterator with loads the data in batches and then we can load the data. Let's view some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_itr = dt.as_numpy_iterator()\n",
    "dt_itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_load = dt_itr.next()\n",
    "len(btc_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here if you see the length of the batch is 2. That is because it will contain images and the label of the image seperately. Hence 2. \n",
    "Now let us check the shape of 0th position of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_load[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is you see there are 4 values. In this 32 stands for the batch size, 256 is the width of the images, another 256 stands for the height of the image and 3 means color value. In this case since the pictures are colored it is 3 (rgb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_load[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the labels with numbers indicating different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_class = dt.class_names\n",
    "eq_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as per the class order,\n",
    "1 = Dumbells\n",
    "2 = Elliptical Machine\n",
    "3 = Home Machine\n",
    "4 = Recumbent Bike\n",
    "\n",
    "Let's check if it is in order before we proceed. To check this I will plot 4 images and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, impt = plt.subplots(nrows=4, ncols=3, figsize= (13,13))\n",
    "\n",
    "impt = impt.flatten()\n",
    "\n",
    "for i, img in enumerate(btc_load[0][2:14]):\n",
    "    impt[i].imshow(img.astype(int))\n",
    "    impt[i].title.set_text(btc_load[1][2:14][i])\n",
    "    impt[i].set_xticks([])\n",
    "    impt[i].set_yticks([])    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in these above pictures the images are correctly catergorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='thoughts'></a>\n",
    "## üí≠ <span style=\"color: #20479b; font-weight: bold;\">My thoughts on the dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per my initial Data Exploration,\n",
    "- I can see that the dataset big enough to build a ML model.\n",
    "- This dataset consited of unnessary files which wouldn't be supported, hence I have removed it.\n",
    "- There is one particular file which is giving me the error of incorrect srgb profile maintainance. I will be trying to find which file is it and deleting that file.\n",
    "- We have loaded the data using keras image from directory function. This has made it very easy.\n",
    "- The data in maintained in two indexes. Index 0 containes the image itself and index 1 contains the labels for the corresponding image.\n",
    "- the sRGB error that we are getting can be ignored but having said that it can have a impact on the model as well hence I will be trying to remove it.\n",
    "- It is difficult to check the image data here in the pipeline, hence I have validated the data in my file explorer as well.\n",
    "- As I will have to train the model and then later test it, I will be splitting our dataset into two parts. Training, Validation and Testing split.\n",
    "\n",
    "\n",
    "Evaluation Metrics:\n",
    "For this dataset I will be using, Accuracy, precision, F1 are the evaluation metrics.\n",
    "\n",
    "\n",
    "\n",
    "Considering all these above points, I will be following the below steps to \n",
    "\n",
    "**‚öôÔ∏è pre-process**\n",
    "1. üîç Find the images which are giving you issues.\n",
    "2. üóëÔ∏è Delete all the images that have issues with the sRGB profile.\n",
    "<!-- 4. ‚ûó Spliting our main datset into two parts, one part for training and another for testing dataframe\n",
    "    - Doing it before testing or \n",
    "    - to avoid data Scalling and oversampling. -->\n",
    "\n",
    "**üîß feature Engineer**\n",
    "1. ‚öñÔ∏è Scalling the numerical and categorical labels\n",
    "\n",
    "<!-- 2. üìà Oversampling the data for stroke = 1, Because of two main reasons.\n",
    "    - Our focus is on stroke detection and data for people who have stroke is very less.\n",
    "    - We do not have to worry about data size as well here, since it's not very huge. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='processing' ></a>\n",
    "## ‚öôÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Pre-processing the data</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. ‚ùé Scaling the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the data is not loaded at once, and it get loaded which processing we cannot scale the whole dataset at once. Hence I will be creating a lamda function to scale the lamda function on load and then we will check if it's working as expected.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scld_dt = dt.map(lambda pictures, label: (pictures/255, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, next when I call function to load a batch it should automatically scale the dataset and load it for us. And to verify it we will be checking the max and min value of the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scld_btc = scld_dt.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Min Values :{scld_btc[0].min()}')\n",
    "print(f'Max Values :{scld_btc[0].max ()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we can confirm that our data is been successfully scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just to check that our image data is preserved even when we scaled the data let's load the pictures again and check if any image data is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, impt = plt.subplots(nrows=4, ncols=3, figsize= (13,13))\n",
    "\n",
    "impt = impt.flatten()\n",
    "\n",
    "for i, img in enumerate(scld_btc[0][2:14]):\n",
    "    impt[i].imshow(img)\n",
    "    impt[i].title.set_text(scld_btc[1][2:14][i])\n",
    "    impt[i].set_xticks([])\n",
    "    impt[i].set_yticks([])    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. ‚ùé Splitting the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scaled the data, we can split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in total there are 25 batches. We can deviled them 7:2:1 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_btc_size = 17\n",
    "validation_btc_size = 5\n",
    "test_btc_size= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_btc_dt = scld_dt.take(train_btc_size)\n",
    "validation_btc_dt = scld_dt.skip(train_btc_size).take(validation_btc_size)\n",
    "test_btc_dt = scld_dt.skip(train_btc_size+validation_btc_size).take(test_btc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total batches in Train      :{len(train_btc_dt)}')\n",
    "print(f'Total batches in Validation :{len(validation_btc_dt)}')\n",
    "print(f'Total batches in Test       :{len(test_btc_dt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature' ></a>\n",
    "## üîß <span style=\"color: #20479b; font-weight: bold;\">Feature Engineering</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature' ></a>\n",
    "## üîß <span style=\"color: #20479b; font-weight: bold;\">Deep Leaning Model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. ‚ùé Building a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = Sequential([\n",
    "     tf.keras.layers.Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3))\n",
    "    ,tf.keras.layers.MaxPooling2D()\n",
    "    ,tf.keras.layers.Conv2D(32, (3,3), 1, activation='relu')\n",
    "    ,tf.keras.layers.MaxPooling2D()\n",
    "    # ,tf.keras.layers.Conv2D(16, (3,3), 1, activation='relu')\n",
    "    # ,tf.keras.layers.MaxPooling2D()\n",
    "    ,tf.keras.layers.Flatten()\n",
    "    ,tf.keras.layers.Dense(256, activation='relu')\n",
    "    ,tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "**Model: \"sequential\"**\n",
    "\n",
    "| Layer (type)                   | Output Shape           | Param #       |\n",
    "|--------------------------------|------------------------|---------------|\n",
    "| conv2d (Conv2D)                | (None, 254, 254, 16)   | 448           |\n",
    "| max_pooling2d (MaxPooling2D)   | (None, 127, 127, 16)   | 0             |\n",
    "| conv2d_1 (Conv2D)              | (None, 125, 125, 32)   | 4,640         |\n",
    "| max_pooling2d_1 (MaxPooling2D) | (None, 62, 62, 32)     | 0             |\n",
    "| flatten (Flatten)              | (None, 123008)         | 0             |\n",
    "| dense (Dense)                  | (None, 256)            | 31,490,304    |\n",
    "| dense_1 (Dense)                | (None, 4)              | 1,028         |\n",
    "\n",
    "**Total params:** 31,496,420 (120.15 MB)  \n",
    "**Trainable params:** 31,496,420 (120.15 MB)  \n",
    "**Non-trainable params:** 0 (0.00 B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = mdl.fit(train_btc_dt, epochs=15, validation_data=validation_btc_dt, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='red', label='train_loss')\n",
    "plt.plot(hist.history['val_loss'], color='blue', label='validation_loss')\n",
    "loss_plot.suptitle('Loss', fontsize=20)\n",
    "loss_plot.legend(loc=\"upper right\")\n",
    "loss_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='red', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='selection'></a>\n",
    "## üóÇÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Experimental Section</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I have 10 different configurations and choose the best out of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  table {\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "    border: 1px solid #ddd;\n",
    "    font-family: Arial, sans-serif;\n",
    "  }\n",
    "  th, td {\n",
    "    padding: 12px;\n",
    "    text-align: left;\n",
    "    border-bottom: 1px solid #ddd;\n",
    "  }\n",
    "  th {\n",
    "    background-color: #03bafc;\n",
    "    color: #000;\n",
    "  }\n",
    "  tr:nth-child(even) {\n",
    "    background-color: #2e2e2e;\n",
    "  }\n",
    "  tr:hover {\n",
    "    background-color: #2d3f47; /* Light tint of blue for hover effect */\n",
    "  }\n",
    "  td:first-child {\n",
    "    font-weight: bold;\n",
    "  }\n",
    "   tr.mdl_config8 {\n",
    "    background-color: #1a5229; /* Dark light green */\n",
    "  } \n",
    "  .mdl_config8 {\n",
    "    background-color: #1a5229; /* Darker green for note */\n",
    "    color: white;\n",
    "    padding: 8px;\n",
    "    font-style: italic;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model Config</th>\n",
    "      <th>Layers</th>\n",
    "      <th>Validation Accuracy</th>\n",
    "      <th>Observations</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><b>mdl_config1</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.8375</td>\n",
    "      <td>The accuracy was very low and it was fluctuating a lot. Hence, I will add one more dense hidden layer to the model.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config2</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(16, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.4312</td>\n",
    "      <td>With the additional layer with 16 neurons, the accuracy dipped to 43%. I wanted to check if making the network more complex would help the model predict better.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config3</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(16, activation='relu')<br>tf.keras.layers.Dense(32, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.63125</td>\n",
    "      <td>With the addition of another layer for 16 neurons, the accuracy increased by approximaterly 20%. Since these are images I wanted to check addition of convolutional and maxpooling layer will help, I will also increase the neurons for the dense layer</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config4</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.93125</td>\n",
    "      <td>Due to the convlutional layers it was able to read the images better. It was able to extract various features from the dataset. The accuracy has not increased to 93. Now I want to check it adding one more layer will help increase the accuracy.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config5</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.93750</td>\n",
    "      <td>There was no significant increace in the accuracy after adding another convolutional layer. But it was still slightly better than the previous configuration. Just to experiment I want to add another layer of convolution with more neurons and see if it increases the performance.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config6</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.99375</td>\n",
    "      <td>This was a huge improvement for me. The accuracy went to 99. The accuracy is fluctuating a little bit though. I do not want the model to overfit the train dataset hence I will experiment adding Early Stopping and see how it impacts</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config7</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.996324</td>\n",
    "      <td>The accuracy has improved slightly and the accuracy curve is also much better. To avoid further overfit I will add Dropout layer see how it impacts</td>\n",
    "    <tr class=\"mdl_config8\">\n",
    "      <td><b>üëë mdl_config8</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td class=\"observations\">The accuracy has decreased slightly, this may be because it was overfitting the data earlier. Let me add batch normalization and check if it will improve the accuracy.\n",
    "      <br><br><b>Note:</b> I have chosen this even if model 7 has better results because this model has both Early Stopping and Dropout, and even with that, it was able to score 0.98750 without overfitting the train data, which is great!!</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config9</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>BatchNormalization()<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td>The accuracy is still the same. Now I want to check if changing the optimizers to sgd from adam will help increase the acccuracy</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config10</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>BatchNormalization()<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td>With sgd the accuracy dropped to 72, and the accuracy fluctuates alot. So I will be going back to the adam optimizer. And I will try increasing the learning rate</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config11</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>BatchNormalization()<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td>With increase in learning rate thethe accuracy fluctuates even more.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  table {\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "    border: 1px solid #ddd;\n",
    "    font-family: Arial, sans-serif;\n",
    "  }\n",
    "  th, td {\n",
    "    padding: 12px;\n",
    "    text-align: left;\n",
    "    border-bottom: 1px solid #ddd;\n",
    "  }\n",
    "  th {\n",
    "    background-color: #03bafc;\n",
    "    color: #000;\n",
    "  }\n",
    "  tr:nth-child(even) {\n",
    "    background-color: #f2f2f2;\n",
    "  }\n",
    "  tr:hover {\n",
    "    background-color: #e6f7ff; /* Light tint of blue for hover effect */\n",
    "  }\n",
    "  td:first-child {\n",
    "    font-weight: bold;\n",
    "  }\n",
    "  /* Specific highlighting for mdl_config8 */\n",
    "  tr.mdl_config8 {\n",
    "    background-color: #c8e6c9; /* Dark light green */\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model Config</th>\n",
    "      <th>Layers</th>\n",
    "      <th>Validation Accuracy</th>\n",
    "      <th>Observations</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><b>mdl_config1</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.8375</td>\n",
    "      <td>The accuracy was very low and it was fluctuating a lot. Hence, I will add one more dense hidden layer to the model.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config2</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(16, activation='relu')<br>Dense(4, activation='softmax')</td>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Config 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start with the basic cofigurations just will input and output layers and then go on improving the pipeline. \n",
    "Let us check it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config1 = Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(256,256,3))\n",
    "    ,tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config1.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "logdir='logs/mdl_config1'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "hist = mdl_config1.fit(train_btc_dt, epochs=10, validation_data=validation_btc_dt, callbacks=[tensorboard_callback])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config1.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist.history['loss'],\n",
    "    'Validation Loss': hist.history['val_loss'],\n",
    "    'Accuracy': hist.history['accuracy'],\n",
    "    'Validation Accuracy': hist.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy seems very low. Let me check if adding another dense layer will help improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Config 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config2 = Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(256,256,3)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config2.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "logdir='logs/mdl_config2'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "hist = mdl_config2.fit(train_btc_dt, epochs=10, validation_data=validation_btc_dt, callbacks=[tensorboard_callback])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config2.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist.history['loss'],\n",
    "    'Validation Loss': hist.history['val_loss'],\n",
    "    'Accuracy': hist.history['accuracy'],\n",
    "    'Validation Accuracy': hist.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one hidden layer and 16 neurons the accuracy is fluctuating a lot. Loss is significantly higher than previous configuration. Let me try adding more neurons and check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Config 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config3 = Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(256,256,3)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config3.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "logdir='logs/mdl_config3'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "hist = mdl_config3.fit(train_btc_dt, epochs=10, validation_data=validation_btc_dt, callbacks=[tensorboard_callback])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config3.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist.history['loss'],\n",
    "    'Validation Loss': hist.history['val_loss'],\n",
    "    'Accuracy': hist.history['accuracy'],\n",
    "    'Validation Accuracy': hist.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Config 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me make the network more complex and try if it improved the performance.\n",
    "I will be adding Conv2D Layer. This will help it extract patterns from the images.\n",
    "And then to reduce dimentionality I will use Max Pooling. I will start with 16 filters and (2,2) is the kernal size and 1 is the stride taken accoss the pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config4 = Sequential([\n",
    "     tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config4.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "logdir='logs/mdl_config4'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "hist_config4 = mdl_config4.fit(train_btc_dt, epochs=10, validation_data=validation_btc_dt, callbacks=[tensorboard_callback])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config4.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config4.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config4.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config4.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config4.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist_config4.history['loss'],\n",
    "    'Validation Loss': hist_config4.history['val_loss'],\n",
    "    'Accuracy': hist_config4.history['accuracy'],\n",
    "    'Validation Accuracy': hist_config4.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done these additions it looks so much better. The loss has come down marginally, and the accurary has gone up and it is arround 95 for train and 93 for validation set. Let me add one more convolution layer and check the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Config 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config5 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config5.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "logdir='logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "hist_config5 = mdl_config5.fit(train_btc_dt, epochs=10, validation_data=validation_btc_dt, callbacks=[tensorboard_callback])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config5.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config5.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config5.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config5.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config5.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist_config5.history['loss'],\n",
    "    'Validation Loss': hist_config5.history['val_loss'],\n",
    "    'Accuracy': hist_config5.history['accuracy'],\n",
    "    'Validation Accuracy': hist_config5.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the accuracy score has gone marginally higher. Now it's peaking at 99 and then there is a slight drop in accuracy after that. Validation score also has the similar pattern where it peaks and then there is a drop we can add one more layer and check if it helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Config 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config6 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config6.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "logdir='logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "hist_config6 = mdl_config6.fit(train_btc_dt, epochs=10, validation_data=validation_btc_dt, callbacks=[tensorboard_callback])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config6.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config6.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config6.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config6.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config6.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist_config6.history['loss'],\n",
    "    'Validation Loss': hist_config6.history['val_loss'],\n",
    "    'Accuracy': hist_config6.history['accuracy'],\n",
    "    'Validation Accuracy': hist_config6.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the accuracy of both the train and validation has improved alot. Even the loss that we are having is marginally very low. Which is a good sign for us. But now I want to increace the epoch to 20 and since the curves look similar we want to avoid overfitting hence I want to add early stopping. Let's see it there is improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7. Config 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config7 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config7.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "# logdir='logs'\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='validation_loss',mode='max',patience=4, restore_best_weights=True)\n",
    "hist_config7 = mdl_config7.fit(train_btc_dt, epochs=20, validation_data=validation_btc_dt, callbacks=[early_stop])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config7.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config7.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config7.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config7.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config7.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist_config7.history['loss'],\n",
    "    'Validation Loss': hist_config7.history['val_loss'],\n",
    "    'Accuracy': hist_config7.history['accuracy'],\n",
    "    'Validation Accuracy': hist_config7.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the performance has improved alot. the accuracy of our model is arround 0.99 and validation accuracy has peaked to 1 which is great.  I will also add dropout layers again for the same reason so that we can avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **8. Config 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config8 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config8.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "# logdir='logs'\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='validation_loss',mode='max',patience=4, restore_best_weights=True)\n",
    "hist_config8 = mdl_config8.fit(train_btc_dt, epochs=20, validation_data=validation_btc_dt, callbacks=[early_stop])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config8.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config8.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config8.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config8.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config8.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist_config8.history['loss'],\n",
    "    'Validation Loss': hist_config8.history['val_loss'],\n",
    "    'Accuracy': hist_config8.history['accuracy'],\n",
    "    'Validation Accuracy': hist_config8.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to see that even with dropout set to half the accuracy of our model has not dropped significantly. Now I want to check if normalizing the batches will help the increase the accuracy or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **9. Config 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config9 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config9.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "# logdir='logs'\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='validation_loss',mode='max',patience=4, restore_best_weights=True)\n",
    "hist_config9 = mdl_config9.fit(train_btc_dt, epochs=20, validation_data=validation_btc_dt, callbacks=[early_stop])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config9.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config9.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config9.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config9.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config9.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist_config9.history['loss'],\n",
    "    'Validation Loss': hist_config9.history['val_loss'],\n",
    "    'Accuracy': hist_config9.history['accuracy'],\n",
    "    'Validation Accuracy': hist_config9.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **10. Config 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config10 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    # tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config10.compile('sgd', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "# logdir='logs'\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='validation_loss',mode='max',patience=4, restore_best_weights=True)\n",
    "hist_config10 = mdl_config10.fit(train_btc_dt, epochs=20, validation_data=validation_btc_dt, callbacks=[early_stop])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config10.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config10.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config10.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config10.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config10.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist_config10.history['loss'],\n",
    "    'Validation Loss': hist_config10.history['val_loss'],\n",
    "    'Accuracy': hist_config10.history['accuracy'],\n",
    "    'Validation Accuracy': hist_config10.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **11. Config 11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config11 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config11.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "# logdir='logs'\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='validation_loss',mode='max',patience=4, restore_best_weights=True)\n",
    "hist_config11 = mdl_config11.fit(train_btc_dt, epochs=20, validation_data=validation_btc_dt, callbacks=[early_stop])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config11.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config11.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config11.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config11.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config11.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist_config11.history['loss'],\n",
    "    'Validation Loss': hist_config11.history['val_loss'],\n",
    "    'Accuracy': hist_config11.history['accuracy'],\n",
    "    'Validation Accuracy': hist_config11.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config1 = Sequential([\n",
    "     tf.keras.layers.Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3))\n",
    "    ,tf.keras.layers.MaxPooling2D()\n",
    "    ,tf.keras.layers.Conv2D(32, (3,3), 1, activation='relu')\n",
    "    # ,tf.keras.layers.MaxPooling2D()\n",
    "    ,tf.keras.layers.Flatten()\n",
    "    # ,tf.keras.layers.Dense(256, activation='relu')\n",
    "    ,tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config1.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "logdir='logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "hist = mdl_config1.fit(train_btc_dt, epochs=3, validation_data=validation_btc_dt, callbacks=[tensorboard_callback])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config1.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame({\n",
    "    'Train Loss': hist.history['loss'],\n",
    "    'Validation Loss': hist.history['val_loss'],\n",
    "    'Accuracy': hist.history['accuracy'],\n",
    "    'Validation Accuracy': hist.history['val_accuracy'],   \n",
    "})\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## ‚úÖ <span style=\"color: #20479b; font-weight: bold;\">Assessing which gives us the best results</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to choose the best-performing model. And the margin can be very very small hence I will create a list. And then I will keep appending the value metric list. I will loop through all the models that I have and then I will print the results according to the valuation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fet_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_met_list = []\n",
    "\n",
    "\n",
    "mdl_list = [log_reg,svc,ran_for_cls,xg_bst,k_n_n]\n",
    "\n",
    "for i, mdl in enumerate(mdl_list):\n",
    "\n",
    "    if mdl==k_n_n:\n",
    "        fet_train_smot_val = fet_train_smot.values\n",
    "        tar_train_smot_val = tar_train_smot.values\n",
    "        fet_test_df_val    = fet_test_df.values\n",
    "    else:\n",
    "        fet_train_smot_val = fet_train_smot\n",
    "        tar_train_smot_val = tar_train_smot\n",
    "        fet_test_df_val    = fet_test_df\n",
    "\n",
    "    tar_pred_df = pd.Series(mdl.predict(\n",
    "                                fet_test_df_val\n",
    "                                ))\n",
    "\n",
    "\n",
    "    val_met_list.append([ \n",
    "                        mdl.__class__.__name__,\n",
    "                        cross_val_score(estimator=mdl, X=fet_train_smot_val, y=tar_train_smot_val, cv=15).mean(), \n",
    "                        accuracy_score(tar_test_df, tar_pred_df),\n",
    "                        roc_auc_score(tar_test_df, tar_pred_df),\n",
    "                        precision_score(tar_test_df, tar_pred_df),\n",
    "                        f1_score(tar_test_df, tar_pred_df)\n",
    "                        ])\n",
    "\n",
    "\n",
    "val_met_df = pd.DataFrame(val_met_list\n",
    "                        ,columns=['Model Name','Avg ACC','ACC','AUC','PRE','F1']\n",
    "                        )\n",
    "val_met_df = val_met_df.sort_values(\n",
    "                                    ascending=False, by=['Avg ACC','ACC','AUC']\n",
    "                                    )\n",
    "val_met_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that Ran forest performed the best will the avegare acccuracy of 97.21, and AUC of 0.50.\n",
    "Xgboost is the second best will the avegare acccuracy of 96.07, and AUC of 0.51.\n",
    "\n",
    "\n",
    "As per our valuation metrics the best model out of them are\n",
    "1. Random Forest\n",
    "2. Xgboost\n",
    "\n",
    "Let us try to tune these two models and try to find we can get the model to perform even better score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tuning'></a>\n",
    "## üéõÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Hyper-Parameter Tuning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will using gridsearchcv and passing it multiple values so that it can choose a best value for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\" font-weight: bold;\">Random Forest Classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion':['gini', 'log_loss'],'n_estimators':[60,80,100,120,140],  'random_state' : [1200,None]}\n",
    "param_result_list = []\n",
    "grid_ser_cv = GridSearchCV(ran_for_cls,params,cv=5)\n",
    "grid_ser_cv.fit(fet_train_smot,tar_train_smot)\n",
    "df_results = pd.DataFrame(grid_ser_cv.cv_results_)\n",
    "param_result_list.append([grid_ser_cv.best_score_,grid_ser_cv.best_params_])\n",
    "\n",
    "param_result_df = pd.DataFrame(param_result_list, columns=['best_score_','best_params_',])\n",
    "param_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\" font-weight: bold;\">Support Vector Classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eta':[0.1,0.4,0.2,0.3],'booster':['dart','gblinear', 'gbtree']}\n",
    "param_result_list = []\n",
    "grid_ser_cv = GridSearchCV(xg_bst,params,cv=5)\n",
    "grid_ser_cv.fit(x_train_smot,y_train_smot)\n",
    "df_results = pd.DataFrame(grid_ser_cv.cv_results_)\n",
    "param_result_list.append([grid_ser_cv.best_score_,grid_ser_cv.best_params_])\n",
    "\n",
    "param_result_df = pd.DataFrame(param_result_list, columns=['best_score_','best_params_',])\n",
    "param_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the scores of `random_forect_classifier` and `svm` model, we can come to a conclusion that both are a great model for this dataset.  \n",
    "Having said that `random_forect_classifier` model perfomed sligtly better. Hence I willl choose that as my final model with `{'criterion': 'gini', 'n_estimators': 80, 'random_state': None}` as the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Final'></a>\n",
    "## üèÅ <span style=\"color: #20479b; font-weight: bold;\">Final Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ran_for_cls =  sklearn.ensemble.RandomForestClassifier(criterion= 'log_loss', n_estimators= 60,  random_state = None)\n",
    "final_ran_for_cls.fit(fet_train_smot,\n",
    "            tar_train_smot\n",
    "            )\n",
    "y_pred_ran_for_cls = final_ran_for_cls.predict(\n",
    "                                         fet_test_df\n",
    "                                         )\n",
    "print('Random Forest')\n",
    "print('='*53)\n",
    "print('AVG ACC }',cross_val_score(estimator=log_reg, X=fet_train_smot, y=tar_train_smot, cv=5).mean())\n",
    "print('PRE     }',precision_score(tar_test_df, y_pred_ran_for_cls))\n",
    "print('ACC     }',accuracy_score(tar_test_df, y_pred_ran_for_cls))\n",
    "print('F1S     }',f1_score(tar_test_df, y_pred_ran_for_cls))\n",
    "print('AUS     }',roc_auc_score(tar_test_df, y_pred_ran_for_cls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## üí° <span style=\"color: #20479b; font-weight: bold;\">Conclusion</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize everythin we did\n",
    "- we started with loading the dataset\n",
    "- In our eda we noticed that\n",
    "    - identity column was unnessary.\n",
    "    - body_mass_index column had null values.\n",
    "    - dataset was pretty imbalanced.\n",
    "- cleaned the data, by deleting null rows, dropping identity column, and split the data to avoid data leakage.\n",
    "- And then we scaled the numerical and categorical values.\n",
    "- oversampled the data using smote.\n",
    "\n",
    "I tried 5 different model. Out of which `random forest classifier` and `xgboost` model performed the best.  \n",
    "I tuned tuned the hyper parameters using `gridsearchCV` and found that amond both the models, `random forest classifier` outperformed `xgboost` slightly.\n",
    "\n",
    "As per the valuation metrics I  found `random forest classifier` with `{'C': 1.4, 'degree': 4, 'kernel': 'poly'}` parameters provided the best with pretty good acccuracy of 93%.\n",
    "\n",
    "With this model in place, now my company can use this model to take inputs from the users and prompt them if there are chances of getting a stroke for them. This solves the problem that we stated before. This will significantly help the users in reducing the rist of strokes, and notify them to get medical help as soon as possible.\n",
    "\n",
    "I felt that if the data has more parametrics we could have improved more. Also `body_mass_index` of a person does impact on the chances of getting an stroke, but in this dataset it did not show the signs of it. Hence it will be better if data collection is done even more accurately to improve the accuracy of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
