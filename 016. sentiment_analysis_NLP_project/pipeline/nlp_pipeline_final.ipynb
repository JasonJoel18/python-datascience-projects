{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 2em;\">\n",
    "  <img src=\"/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/016. sentiment_analysis_NLP_project/dataset/icons8-twitter-pastel/icons8-twitter-256.png\" alt=\"Your Image\" style=\"height: 2.5em; margin-right: 15px;\">\n",
    "  Twitter Post Classification\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëãüèº **Introduction**\n",
    "\n",
    "<p style=\"text-align: justify;\">I am a data scientist. And my company wants me to build and NLP pipeline where it wants me to categorize my data as racist/sexist or not. The problem that currently the company is facing is there are alot of racist tweets on being posted online. And this causes a lot of distress among the other users, and there are potenial chances of them stop using the app due to these issues. Hence, we want to report these tweets and taken down as early as possible.</p>\n",
    "\n",
    "## üßø **Objective**\n",
    " <p style=\"text-align: justify;\">The objective of this project is that I will have to build a module which will try to classify the category of twitter reviews. For this my company has given me a dataset which is in csv format. It has three columns, id, tweet and the label. So I will be using these data in the csv file and building an NLP model ???????. Firstly I will be going through the full csv dataset and trying to remove the unwanted data if unsupported extensions. And then using these data I will be building a model. I will also be doing several experiments in order to improve the performance of the model. I will check if our dataset is balanced and if there is a requirement to increase the dataset so that our model can learn better from the data. Then I will choose the best-performing model to build my final model with the best-suited parameters.\n",
    " </p>\n",
    "\n",
    "**About the data**  \n",
    "This dataset contains 12 columns. This data set is from Kaggle. You can [**click here**](https://www.kaggle.com/datasets/mayurdalvi/twitter-sentiments-analysis-nlp) to view the dataset on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Content**\n",
    "\n",
    "##### ‚¨á [**Importing Libraries**](#library)\n",
    "##### üìä [**Importing Dataset**](#dataset)\n",
    "##### üó∫Ô∏è [**Exploring the dataset**](#exploration)\n",
    "##### üí≠ [**My taughts on the dataset**](#thoughts)\n",
    "##### ‚öôÔ∏è [**Pre-processing the data**](#processing)\n",
    "##### üîß [**Feature Engineering**](#feature)\n",
    "##### üóÇÔ∏è [**Selecting the best model for our dataset**](#selection)\n",
    "##### ‚úÖ [**Assessing which gives us the best results**](#assess)\n",
    "##### üéõÔ∏è [**Hyper-Parameter Tuning**](#tuning)\n",
    "##### üèÅ [**Final Model**](#Final)\n",
    "##### üí° [**Conclusion**](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Let's begin....***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='library'></a>\n",
    "## ‚¨á <span style=\"color: #20479b; font-weight: bold;\">Importing Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing the necessary libraries for my Exploratory Data Analysis tasks in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import  pandas as  pd\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>  \n",
    "\n",
    "## üìä <span style=\"color: #20479b; font-weight: bold;\">Importing Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst_dir = '/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/016. sentiment_analysis_NLP_project/dataset/Twitter Sentiments.csv'\n",
    "dtst = pd.read_csv(dtst_dir)\n",
    "print(display(dtst.sample(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## üó∫Ô∏è <span style=\"color: #20479b; font-weight: bold;\">Exploring the dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(display(dtst.sample(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 31962 rows of data which is good so that the model can learn better from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as we can see there are three columns i.e. id, label and tweet.\n",
    "As per the data authors, 1 here stands for rasist/sexist tweet and 0 stands for non racist comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentage = {}\n",
    "for column in dtst:\n",
    "    null_percentage[column] = round(( ( dtst[column].isnull().sum() ) / len(dtst)) * 100,2)\n",
    "np_dataset = pd.DataFrame( list( null_percentage.items() ),  columns=[  'column_name'  , 'null_percentage' ])\n",
    "np_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null entries in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no unnessary entries too. Which is a good thing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ct = dtst['label'].value_counts()\n",
    "print('Each class count:')\n",
    "display(t_ct)\n",
    "print('1 class % is :',round(100*(t_ct[1]/(t_ct[0]+t_ct[1]))),'%')\n",
    "sns.barplot(x=t_ct.index, y=t_ct.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "For EDA I will be using the main data. Once data is normalized, I will be splitting it into `train` and `test` split.\n",
    "1. Screening the images in the pipeline one by one takes a lot of time. Hence we will be looking at only few of the images and making sure that it correctly maintained.\n",
    "2. I will be splitting the data after normalization because otherwise, the three datasets won't be even. \n",
    "\n",
    "Here one disadvantage that I saw is that we cannot view the image randomly. We need a iterator with loads the data in batches and then we can load the data. Let's view some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_itr = dt.as_numpy_iterator()\n",
    "dt_itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_load = dt_itr.next()\n",
    "len(btc_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here if you see the length of the batch is 2. That is because it will contain images and the label of the image seperately. Hence 2. \n",
    "Now let us check the shape of 0th position of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_load[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is you see there are 4 values. In this 32 stands for the batch size, 256 is the width of the images, another 256 stands for the height of the image and 3 means color value. In this case since the pictures are colored it is 3 (rgb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_load[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the labels with numbers indicating different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_class = dt.class_names\n",
    "eq_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as per the class order,\n",
    "1 = Dumbells\n",
    "2 = Elliptical Machine\n",
    "3 = Home Machine\n",
    "4 = Recumbent Bike\n",
    "\n",
    "Let's check if it is in order before we proceed. To check this I will plot 4 images and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, impt = plt.subplots(nrows=4, ncols=3, figsize= (13,13))\n",
    "\n",
    "impt = impt.flatten()\n",
    "\n",
    "for i, img in enumerate(btc_load[0][2:14]):\n",
    "    impt[i].imshow(img.astype(int))\n",
    "    impt[i].title.set_text(btc_load[1][2:14][i])\n",
    "    impt[i].set_xticks([])\n",
    "    impt[i].set_yticks([])    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in these above pictures the images are correctly catergorized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='thoughts'></a>\n",
    "## üí≠ <span style=\"color: #20479b; font-weight: bold;\">My thoughts on the dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per my initial Data Exploration,\n",
    "- I can see that the dataset big enough to build a ML model.\n",
    "- This dataset consited of unnessary files which wouldn't be supported, hence I have removed it.\n",
    "- There is one particular file which is giving me the error of incorrect srgb profile maintainance. I will be trying to find which file is it and deleting that file.\n",
    "- We have loaded the data using keras image from directory function. This has made it very easy.\n",
    "- The data in maintained in two indexes. Index 0 containes the image itself and index 1 contains the labels for the corresponding image.\n",
    "- the sRGB error that we are getting can be ignored but having said that it can have a impact on the model as well hence I will be trying to remove it.\n",
    "- It is difficult to check the image data here in the pipeline, hence I have validated the data in my file explorer as well.\n",
    "- As I will have to train the model and then later test it, I will be splitting our dataset into two parts. Training, Validation and Testing split.\n",
    "\n",
    "\n",
    "Evaluation Metrics:\n",
    "For this dataset I will be using, Accuracy, precision, F1 are the evaluation metrics.\n",
    "\n",
    "\n",
    "\n",
    "Considering all these above points, I will be following the below steps to \n",
    "\n",
    "**‚öôÔ∏è pre-process**\n",
    "1. üîç Find the images which are giving you issues.\n",
    "2. üóëÔ∏è Delete all the images that have issues with the sRGB profile.\n",
    "<!-- 4. ‚ûó Spliting our main datset into two parts, one part for training and another for testing dataframe\n",
    "    - Doing it before testing or \n",
    "    - to avoid data Scalling and oversampling. -->\n",
    "\n",
    "**üîß feature Engineer**\n",
    "1. ‚öñÔ∏è Scalling the numerical and categorical labels\n",
    "\n",
    "<!-- 2. üìà Oversampling the data for stroke = 1, Because of two main reasons.\n",
    "    - Our focus is on stroke detection and data for people who have stroke is very less.\n",
    "    - We do not have to worry about data size as well here, since it's not very huge. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='processing' ></a>\n",
    "## ‚öôÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Pre-processing the data</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. ‚ùé Scaling the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the data is not loaded at once, and it get loaded which processing we cannot scale the whole dataset at once. Hence I will be creating a lamda function to scale the lamda function on load and then we will check if it's working as expected.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scld_dt = dt.map(lambda pictures, label: (pictures/255, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, next when I call function to load a batch it should automatically scale the dataset and load it for us. And to verify it we will be checking the max and min value of the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scld_btc = scld_dt.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Min Values :{scld_btc[0].min()}')\n",
    "print(f'Max Values :{scld_btc[0].max ()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we can confirm that our data is been successfully scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just to check that our image data is preserved even when we scaled the data let's load the pictures again and check if any image data is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, impt = plt.subplots(nrows=4, ncols=3, figsize= (13,13))\n",
    "\n",
    "impt = impt.flatten()\n",
    "\n",
    "for i, img in enumerate(scld_btc[0][2:14]):\n",
    "    impt[i].imshow(img)\n",
    "    impt[i].title.set_text(scld_btc[1][2:14][i])\n",
    "    impt[i].set_xticks([])\n",
    "    impt[i].set_yticks([])    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. ‚ùé Splitting the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scaled the data, we can split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in total there are 25 batches. We can deviled them 7:2:1 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_btc_size = 17\n",
    "validation_btc_size = 5\n",
    "test_btc_size= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_btc_dt = scld_dt.take(train_btc_size)\n",
    "validation_btc_dt = scld_dt.skip(train_btc_size).take(validation_btc_size)\n",
    "test_btc_dt = scld_dt.skip(train_btc_size+validation_btc_size).take(test_btc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total batches in Train      :{len(train_btc_dt)}')\n",
    "print(f'Total batches in Validation :{len(validation_btc_dt)}')\n",
    "print(f'Total batches in Test       :{len(test_btc_dt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature' ></a>\n",
    "## üîß <span style=\"color: #20479b; font-weight: bold;\">Feature Engineering</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='selection'></a>\n",
    "## üóÇÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Experimental Section</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I have 10 different configurations and choose the best out of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  table {\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "    border: 1px solid #ddd;\n",
    "    font-family: Arial, sans-serif;\n",
    "  }\n",
    "  th, td {\n",
    "    padding: 12px;\n",
    "    text-align: left;\n",
    "    border-bottom: 1px solid #ddd;\n",
    "  }\n",
    "  th {\n",
    "    background-color: #03bafc;\n",
    "    color: #000;\n",
    "  }\n",
    "  tr:nth-child(even) {\n",
    "    background-color: #EEEEEE;\n",
    "  }\n",
    "  tr:hover {\n",
    "    background-color: #DBECFA; /* Light tint of blue for hover effect */\n",
    "  }\n",
    "  td:first-child {\n",
    "    font-weight: bold;\n",
    "  }\n",
    "   tr.mdl_config8 {\n",
    "    background-color: #95CD97; /* Dark light green */\n",
    "  } \n",
    "  .mdl_config8 {\n",
    "    background-color: #95CD97; /* Darker green for note */\n",
    "    padding: 8px;\n",
    "    font-style: italic;\n",
    "\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model Config</th>\n",
    "      <th>Layers</th>\n",
    "      <th>Validation Accuracy</th>\n",
    "      <th>Observations</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><b>mdl_config1</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.8375</td>\n",
    "      <td>The accuracy was very low and it was fluctuating a lot. Hence, I will add one more dense hidden layer to the model.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config2</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(16, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.4312</td>\n",
    "      <td>With the additional layer with 16 neurons, the accuracy dipped to 43%. I wanted to check if making the network more complex would help the model predict better.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config3</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(16, activation='relu')<br>tf.keras.layers.Dense(32, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.63125</td>\n",
    "      <td>With the addition of another layer for 16 neurons, the accuracy increased by approximaterly 20%. Since these are images I wanted to check addition of convolutional and maxpooling layer will help, I will also increase the neurons for the dense layer</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config4</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.93125</td>\n",
    "      <td>Due to the convlutional layers it was able to read the images better. It was able to extract various features from the dataset. The accuracy has not increased to 93. Now I want to check it adding one more layer will help increase the accuracy.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config5</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.93750</td>\n",
    "      <td>There was no significant increace in the accuracy after adding another convolutional layer. But it was still slightly better than the previous configuration. Just to experiment I want to add another layer of convolution with more neurons and see if it increases the performance.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config6</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.99375</td>\n",
    "      <td>This was a huge improvement for me. The accuracy went to 99. The accuracy is fluctuating a little bit though. I do not want the model to overfit the train dataset hence I will experiment adding Early Stopping and see how it impacts</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config7</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.996324</td>\n",
    "      <td>The accuracy has improved slightly and the accuracy curve is also much better. To avoid further overfit I will add Dropout layer see how it impacts</td>\n",
    "    <tr class=\"mdl_config8\">\n",
    "      <td><b>üëë mdl_config8</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td class=\"observations\">The accuracy has decreased slightly, this may be because it was overfitting the data earlier. Let me add batch normalization and check if it will improve the accuracy.\n",
    "      <br><br><b>Note:</b> I have chosen this even if model 7 has better results because this model has both Early Stopping and Dropout, and even with that, it was able to score 0.98750 without overfitting the train data, which is great!!</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config9</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>BatchNormalization()<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td>The accuracy is still the same. Now I want to check if changing the optimizers to sgd from adam will help increase the acccuracy</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config10</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>BatchNormalization()<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td>With sgd the accuracy dropped to 72, and the accuracy fluctuates alot. So I will be going back to the adam optimizer. And I will try increasing the learning rate</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config11</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>BatchNormalization()<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td>With increase in learning rate thethe accuracy fluctuates even more.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Final'></a>\n",
    "## üèÅ <span style=\"color: #20479b; font-weight: bold;\">Final Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config8 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config8.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='validation_loss',mode='max',patience=4, restore_best_weights=True)\n",
    "hist_config8 = mdl_config8.fit(train_btc_dt, epochs=20, validation_data=validation_btc_dt, callbacks=[early_stop])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config8.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config8.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config8.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config8.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config8.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Evaluation'></a>\n",
    "## üí° <span style=\"color: #20479b; font-weight: bold;\"> Model Evaluation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "recl_lst = []\n",
    "prcn_lst = []\n",
    "acry_lst = []\n",
    "\n",
    "imgs = []\n",
    "tarpreds = []\n",
    "tars = []\n",
    "\n",
    "for btc in test_btc_dt.as_numpy_iterator():\n",
    "    img, tar = btc\n",
    "    tarpred = mdl_config8.predict(img)\n",
    "    tarpred = tarpred.argmax(axis=1)\n",
    "    recln = recall_score(tar, tarpred, average='macro')\n",
    "    prcnn = precision_score(tar, tarpred, average='macro')\n",
    "    acryn = accuracy_score(tar, tarpred)\n",
    "    prcn_lst.append(prcnn)\n",
    "    recl_lst.append(recln)\n",
    "    acry_lst.append(acryn)\n",
    "\n",
    "    # Append images, predictions, and ground truths for visualization\n",
    "    imgs.extend(img)\n",
    "    tarpreds.extend(tarpred)\n",
    "    tars.extend(tar)\n",
    "\n",
    "    # Break the loop if enough images are collected\n",
    "    if len(imgs) >= 6:\n",
    "        break\n",
    "\n",
    "print(f'Precision : {sum(prcn_lst)/len(prcn_lst)}')\n",
    "print(f'Recall    : {sum(recl_lst)/len(recl_lst)}')\n",
    "print(f'Accuracy  : {sum(acry_lst)/len(acry_lst)}')\n",
    "\n",
    "fig, am = plt.subplots(2, 3, figsize=(12, 8))\n",
    "am = am.ravel()\n",
    "\n",
    "for i,img in enumerate(imgs):\n",
    "    # Plot image\n",
    "    am[i].imshow(img)\n",
    "    am[i].set_xticks([])\n",
    "    am[i].set_yticks([])\n",
    "    \n",
    "    if tarpreds[i] == tar[i]:\n",
    "        ttl = 'green'\n",
    "    else:\n",
    "        ttl = 'red'\n",
    "    \n",
    "    am[i].set_title(f'Predicted: {eq_class[tarpreds[i]]}\\nActual: {eq_class[tars[i]]}', color=ttl)\n",
    "\n",
    "    if i >=5:\n",
    "        break\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plot, we were able to get very good scored and out of the 6 images we were able to predict most of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## üí° <span style=\"color: #20479b; font-weight: bold;\">Conclusion</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://icons8.com/icon/68193/twitter\">Twitter</a> icon by <a target=\"_blank\" href=\"https://icons8.com\">Icons8</a>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
