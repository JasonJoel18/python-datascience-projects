{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 2em;\">\n",
    "  <img src=\"/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/016. sentiment_analysis_NLP_project/dataset/icons8-twitter-pastel/icons8-twitter-256.png\" alt=\"Your Image\" style=\"height: 2.5em; margin-right: 15px;\">\n",
    "  Twitter Post Classification\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëãüèº **Introduction**\n",
    "\n",
    "<p style=\"text-align: justify;\">I am a data scientist. And my company wants me to build and NLP pipeline where it wants me to categorize my data as racist/sexist or not. The problem that currently the company is facing is there are alot of racist tweets on being posted online. And this causes a lot of distress among the other users, and there are potenial chances of them stop using the app due to these issues. Hence, we want to report these tweets and taken down as early as possible.</p>\n",
    "\n",
    "## üßø **Objective**\n",
    " <p style=\"text-align: justify;\">The objective of this project is that I will have to build a module which will try to classify the category of tweets. For this my company has given me a dataset which is in csv format. It has two columns, tweet and the suicide. So I will be using these data in the csv file and building an NLP model. Firstly I will be going through the full csv dataset and trying to remove the unwanted data. And then using these data I will be building a model. I will also be doing several experiments in order to improve the performance of the model. I will check if our dataset is balanced and if there is a requirement to increase the dataset so that our model can learn better from the data. Then I will choose the best-performing model to build my final model with the best-suited parameters.\n",
    " </p>\n",
    "\n",
    "**About the data**  \n",
    "This dataset contains 2 columns. This data set is from Kaggle. You can [**click here**](https://www.kaggle.com/datasets/aunanya875/suicidal-tweet-detection-dataset) to view the dataset on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Content**\n",
    "\n",
    "##### ‚¨á [**Importing Libraries**](#library)\n",
    "##### üìä [**Importing Dataset**](#dataset)\n",
    "##### üó∫Ô∏è [**Exploring the dataset**](#exploration)\n",
    "##### üí≠ [**My taughts on the dataset**](#thoughts)\n",
    "##### ‚öôÔ∏è [**Pre-processing the data**](#processing)\n",
    "##### üîß [**Feature Engineering**](#feature)\n",
    "##### üóÇÔ∏è [**Selecting the best model for our dataset**](#selection)\n",
    "##### ‚úÖ [**Assessing which gives us the best results**](#assess)\n",
    "##### üéõÔ∏è [**Hyper-Parameter Tuning**](#tuning)\n",
    "##### üèÅ [**Final Model**](#Final)\n",
    "##### üí° [**Conclusion**](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Let's begin....***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='library'></a>\n",
    "## ‚¨á <span style=\"color: #20479b; font-weight: bold;\">Importing Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing the necessary libraries for my Exploratory Data Analysis tasks in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import  pandas as  pd\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.model_selection\n",
    "from  imblearn.over_sampling import SMOTE\n",
    "from  imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import optuna\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>  \n",
    "\n",
    "## üìä <span style=\"color: #20479b; font-weight: bold;\">Importing Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst_dir = '/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/016. sentiment_analysis_NLP_project/dataset/Suicide_Ideation_Dataset(Twitter-based).csv'\n",
    "dtst = pd.read_csv(dtst_dir,nrows=10000)\n",
    "print(display(dtst.sample(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## üó∫Ô∏è <span style=\"color: #20479b; font-weight: bold;\">Exploring the dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that \n",
    "1. ID column is not necessary for us, hence we can delete it.\n",
    "2. tweets have alot of unnessary special characters, which we have to clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1787 rows of data which is good so that the model can learn better from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentage = {}\n",
    "for column in dtst:\n",
    "    null_percentage[column] = round(( ( dtst[column].isnull().sum() ) / len(dtst)) * 100,2)\n",
    "np_dataset = pd.DataFrame( list( null_percentage.items() ),  columns=[  'column_name'  , 'null_percentage' ])\n",
    "np_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are null entries in our dataset in tweet column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst[\"Suicide\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no unnessary entries too. Which is a good thing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ct = dtst['Suicide'].value_counts()\n",
    "print('Each class count:')\n",
    "sns.barplot(x=t_ct.index, y=t_ct.values)\n",
    "t_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our dataset is ** imbalanced**. We need to balance the dataset so that we can have good model trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='thoughts'></a>\n",
    "## üí≠ <span style=\"color: #20479b; font-weight: bold;\">My thoughts on the dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per my initial Data Exploration,\n",
    "- There are null values.\n",
    "- I can see that the dataset big enough to build a good ML model.\n",
    "- I need to rename the suicide column to 0 and 1 \n",
    "- I will be also restructuring the column and moving the labels to the end to ease of reading.\n",
    "- The dataset consists of 31962 rows of data.\n",
    "- There are usertags and special characters in the column tweet. We need to remove those.\n",
    "- The dataset is heavily imbalanced. Hence we need to oversample the data.\n",
    "\n",
    "\n",
    "Evaluation Metrics:\n",
    "For this dataset I will be using, Accuracy, precision, F1 are the evaluation metrics.\n",
    "\n",
    "Considering all these above points, I will be following the below steps to \n",
    "\n",
    "**‚öôÔ∏è pre-process**\n",
    "1. üóëÔ∏è Delete the column ID.\n",
    "2. ü´∏üèª Move the column label to the end.\n",
    "3. üîÑ Convert the body to lower case\n",
    "4. üßπ Remove usertags.\n",
    "5. üßπ Remove special characters.\n",
    "<!-- 4. ‚ûó Spliting our main datset into two parts, one part for training and another for testing dataframe\n",
    "    - Doing it before testing or \n",
    "    - to avoid data Scalling and oversampling. -->\n",
    "\n",
    "**üîß feature Engineer**\n",
    "<!-- 1. ‚öñÔ∏è Scalling the numerical and categorical labels -->\n",
    "\n",
    "<!-- 2. üìà Oversampling the data for stroke = 1, Because of two main reasons.\n",
    "    - Our focus is on stroke detection and data for people who have stroke is very less.\n",
    "    - We do not have to worry about data size as well here, since it's not very huge. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='processing' ></a>\n",
    "## ‚öôÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Pre-processing the data</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. üóëÔ∏è Delete the null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst = dtst.dropna()\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Change the column values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['Suicide'] = dtst['Suicide'].str.lower()\n",
    "dtst['Suicide'] = dtst['Suicide'].str.replace(' ', '', regex=False)\n",
    "def chg_col_vl(vl):\n",
    "    if vl == 'notsuicidepost':\n",
    "        return 0\n",
    "    elif vl == 'potentialsuicidepost':\n",
    "        return 1\n",
    "    \n",
    "dtst['Suicide'] =  dtst['Suicide'].apply(chg_col_vl)\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. üîÑ Convert the body to lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst['Tweet'].str.lower()\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. #Ô∏è‚É£ Remove hashtags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst[\n",
    "                             'tweet_processed'\n",
    "                             ].replace(\n",
    "                                        r'#([^\\s]+)', r'\\1', regex=True\n",
    "                                        )\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.  üßπ Remove usertags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst['tweet_processed'].replace(r'@', '', regex=True)\n",
    "dtst.head(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.üóëÔ∏è Remove Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punc(tweet):\n",
    "    tweet = tweet.translate(str.maketrans('','',string.punctuation))\n",
    "    return tweet\n",
    "\n",
    "dtst['tweet_processed'] = dtst['tweet_processed'].apply(rem_punc)\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. üî¢ Clear all numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst[\n",
    "                            'tweet_processed'\n",
    "                            ].replace(\n",
    "                                        r'\\d+', '', regex=True\n",
    "                                        )\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. üßπ Remove special characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst['tweet_processed'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "dtst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Remove unwanted giberish characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_gib_space(twt):\n",
    "    twt = twt.encode('ascii', 'ignore').decode('ascii')\n",
    "    twt = re.sub(r'\\s+', ' ', twt).strip()\n",
    "    return twt\n",
    "\n",
    "dtst['tweet_processed'] = dtst['tweet_processed'].apply(rem_gib_space)\n",
    "dtst.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessor(text):\n",
    "#     text = word_tokenize(text.lower())\n",
    "#     text = [PorterStemmer().stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "#     text = ' '.join(text)\n",
    "#     return text\n",
    "\n",
    "# dtst['tweet_processed'] = dtst['tweet_processed'].apply(preprocessor)\n",
    "\n",
    "dtst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# porter = nltk.SnowballStemmer('english')\n",
    "\n",
    "# def preprocessor(input_text):\n",
    "#     tokenized_text = nltk.word_tokenize(input_text, language='english')\n",
    "#     stems = [porter.stem(t) for t in tokenized_text]\n",
    "#     return ' '.join(stems)\n",
    "\n",
    "# dtst['tweet_processed'] = dtst['tweet_processed'].apply(preprocessor)\n",
    "\n",
    "# dtst.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Creating a TD-IDF matrix for the tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtst['tweet_processed'] = dtst['tweet_processed'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trm_mdl = TfidfVectorizer(min_df=2)\n",
    "m3 = trm_mdl.fit_transform(dtst['tweet_processed'])\n",
    "inf = pd.DataFrame(m3.toarray(), index=dtst.index)\n",
    "dtst = dtst.join(inf)\n",
    "inverted_index = {id: term for term, id in trm_mdl.vocabulary_.items()}\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are unwanted characters in my tweets data. How can I remove those? Here is some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. ‚ùé Splitting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = dtst['Suicide'].values\n",
    "tr = tr.astype(float)\n",
    "\n",
    "ft = dtst.drop(columns=[\n",
    "                        'Tweet',\n",
    "                        'tweet_processed',\n",
    "                        'Suicide'\n",
    "                        ])\n",
    "ft\n",
    "# ft = vectorizer.fit_transform(dtst['tweet_processed']).toarray()\n",
    "# ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'No of features     : {ft.shape[1]}')\n",
    "print(f'Total rows of data : {ft.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_lst = []\n",
    "# for itm in ft.columns:\n",
    "#     if itm in inverted_index:\n",
    "#         ft_lst.append(inverted_index[itm])\n",
    "#     else:\n",
    "#         ft_lst.append(itm)\n",
    "\n",
    "# ft = ft.values\n",
    "# ft = ft.astype('float')\n",
    "# ft_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train_df,  ft_val_test_df,   tr_train_df,  tar_val_test_df = sklearn.model_selection.train_test_split(  ft , tr  ,  test_size = 0.35, shuffle=True, stratify=tr, random_state  =  40)\n",
    "ft_val_df,  ft_test_df,   tar_val_df,  tar_test_df = sklearn.model_selection.train_test_split(  ft_val_test_df , tar_val_test_df  ,  test_size = 0.50,shuffle=True, stratify=tar_val_test_df,random_state  =  40)\n",
    "\n",
    "print(f'Train features shape     : {ft_train_df.shape}')\n",
    "print(f'Train target shape       : {tr_train_df.shape}')\n",
    "print(39*'=')\n",
    "print(f'Validation features shape: {ft_val_df.shape}')\n",
    "print(f'Validation target shape  : {tar_val_df.shape}')\n",
    "print(39*'=')\n",
    "print(f'Test features shape      : {ft_test_df.shape}')\n",
    "print(f'Test target shape        : {tar_test_df.shape}')\n",
    "print(39*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature' ></a>\n",
    "## üîß <span style=\"color: #20479b; font-weight: bold;\">Feature Engineering</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. üìà Oversampling the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **SMOTE** oversampling to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_smt =  SMOTE(  )\n",
    "ft_train_df_smt, tr_train_df_smt   =    mod_smt.fit_resample(\n",
    "                                                            ft_train_df, \n",
    "                                                            tr_train_df)\n",
    "\n",
    "print(\"fet_train_smot size :\",ft_train_df_smt.shape)\n",
    "print(\"tar_train_smot size :\",tr_train_df_smt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **ADASYN** oversampling to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_adc =  ADASYN(  )\n",
    "ft_train_df_adc, tr_train_df_adc   =    mod_adc.fit_resample(\n",
    "                                                            ft_train_df, \n",
    "                                                            tr_train_df)\n",
    "\n",
    "print(\"fet_train_smot size :\",ft_train_df_adc.shape)\n",
    "print(\"tar_train_smot size :\",tr_train_df_adc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='selection'></a>\n",
    "## üóÇÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Selecting the best model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = sklearn.linear_model.LogisticRegression()\n",
    "svc = sklearn.svm.SVC()\n",
    "ran_for_cls =  sklearn.ensemble.RandomForestClassifier()\n",
    "xg_bst = XGBClassifier()\n",
    "# k_n_n =   sklearn.neighbors.KNeighborsClassifier()\n",
    "# gb = sklearn.ensemble.GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(ml, trf, trl, vlf, vll, mtd):\n",
    "        ml.fit(trf, trl)\n",
    "        p = pd.Series(ml.predict(vlf))\n",
    "        return [\n",
    "            mtd,\n",
    "            ml.__class__.__name__,\n",
    "            geometric_mean_score(vll, p),\n",
    "            f1_score(vll, p),\n",
    "            precision_score(vll, p),\n",
    "            recall_score(vll, p),\n",
    "            accuracy_score(vll, p)\n",
    "\n",
    "        ]\n",
    "\n",
    "mt_vl_lts = []\n",
    "lt = [log_reg,svc,ran_for_cls,xg_bst]\n",
    "cmb = [\n",
    "    ('IM', ft_train_df, tr_train_df),\n",
    "    ('SM', ft_train_df_smt, tr_train_df_smt),\n",
    "    ('AD', ft_train_df_adc, tr_train_df_adc)\n",
    "]\n",
    "for ml in lt:\n",
    "    for mtd, trf, trl in cmb:\n",
    "        mt_vl_lt = evaluate_model(ml, trf, trl, ft_val_df, tar_val_df, mtd)\n",
    "        if mt_vl_lt:\n",
    "            mt_vl_lts.append(mt_vl_lt)\n",
    "mt_vl_lts = pd.DataFrame(mt_vl_lts, columns=['Method', 'Model Name', 'G-mean','F1','PRE','REC', 'ACC'])\n",
    "\n",
    "# Sort the DataFrame\n",
    "mt_vl_lts = mt_vl_lts.sort_values(ascending=False, by=['G-mean','F1','PRE','REC', 'ACC'])\n",
    "\n",
    "# Display the results\n",
    "mt_vl_lts.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Hyper'></a>\n",
    "## üéõÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Hyperparameter Tuning</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'booster': 'gbtree',\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.7, 0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.5, 0.7, 0.9, 1.0]),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(**param, use_label_encoder=False)\n",
    "    model.fit(ft_train_df_adc, tr_train_df_adc)  # Ensure ft_train_df_adc and tr_train_df_adc are defined and contain the correct data\n",
    "\n",
    "    preds = model.predict(ft_val_df)  # Ensure ft_val_df is defined and contains the validation features\n",
    "\n",
    "    gmean = geometric_mean_score(tar_val_df, preds)  # Ensure tar_val_df is defined and contains the validation labels\n",
    "    f1 = f1_score(tar_val_df, preds)\n",
    "    precision = precision_score(tar_val_df, preds)\n",
    "    recall = recall_score(tar_val_df, preds)\n",
    "    \n",
    "    return (gmean + f1 + precision + recall) / 4  # This is just an example. You can weight these metrics as you prefer.\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the results of the best trial\n",
    "print(f'Best trial: {study.best_trial.value}')\n",
    "print(f'Best parameters: {study.best_trial.params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "best_model = XGBClassifier(**best_params, use_label_encoder=False)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "gmean = geometric_mean_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "\n",
    "print(f'G-Mean Score: {gmean}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Final'></a>\n",
    "## üèÅ <span style=\"color: #20479b; font-weight: bold;\">Final Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config8 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config8.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='validation_loss',mode='max',patience=4, restore_best_weights=True)\n",
    "hist_config8 = mdl_config8.fit(train_btc_dt, epochs=20, validation_data=validation_btc_dt, callbacks=[early_stop])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config8.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config8.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config8.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config8.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config8.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Evaluation'></a>\n",
    "## üí° <span style=\"color: #20479b; font-weight: bold;\"> Model Evaluation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "recl_lst = []\n",
    "prcn_lst = []\n",
    "acry_lst = []\n",
    "\n",
    "imgs = []\n",
    "tarpreds = []\n",
    "tars = []\n",
    "\n",
    "for btc in test_btc_dt.as_numpy_iterator():\n",
    "    img, tar = btc\n",
    "    tarpred = mdl_config8.predict(img)\n",
    "    tarpred = tarpred.argmax(axis=1)\n",
    "    recln = recall_score(tar, tarpred, average='macro')\n",
    "    prcnn = precision_score(tar, tarpred, average='macro')\n",
    "    acryn = accuracy_score(tar, tarpred)\n",
    "    prcn_lst.append(prcnn)\n",
    "    recl_lst.append(recln)\n",
    "    acry_lst.append(acryn)\n",
    "\n",
    "    # Append images, predictions, and ground truths for visualization\n",
    "    imgs.extend(img)\n",
    "    tarpreds.extend(tarpred)\n",
    "    tars.extend(tar)\n",
    "\n",
    "    # Break the loop if enough images are collected\n",
    "    if len(imgs) >= 6:\n",
    "        break\n",
    "\n",
    "print(f'Precision : {sum(prcn_lst)/len(prcn_lst)}')\n",
    "print(f'Recall    : {sum(recl_lst)/len(recl_lst)}')\n",
    "print(f'Accuracy  : {sum(acry_lst)/len(acry_lst)}')\n",
    "\n",
    "fig, am = plt.subplots(2, 3, figsize=(12, 8))\n",
    "am = am.ravel()\n",
    "\n",
    "for i,img in enumerate(imgs):\n",
    "    # Plot image\n",
    "    am[i].imshow(img)\n",
    "    am[i].set_xticks([])\n",
    "    am[i].set_yticks([])\n",
    "    \n",
    "    if tarpreds[i] == tar[i]:\n",
    "        ttl = 'green'\n",
    "    else:\n",
    "        ttl = 'red'\n",
    "    \n",
    "    am[i].set_title(f'Predicted: {eq_class[tarpreds[i]]}\\nActual: {eq_class[tars[i]]}', color=ttl)\n",
    "\n",
    "    if i >=5:\n",
    "        break\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plot, we were able to get very good scored and out of the 6 images we were able to predict most of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## üí° <span style=\"color: #20479b; font-weight: bold;\">Conclusion</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://icons8.com/icon/68193/twitter\">Twitter</a> icon by <a target=\"_blank\" href=\"https://icons8.com\">Icons8</a>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
