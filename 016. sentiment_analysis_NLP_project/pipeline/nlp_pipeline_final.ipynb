{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 2em;\">\n",
    "  <img src=\"/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/016. sentiment_analysis_NLP_project/dataset/icons8-twitter-pastel/icons8-twitter-256.png\" alt=\"Your Image\" style=\"height: 2.5em; margin-right: 15px;\">\n",
    "  Twitter Post Classification\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëãüèº **Introduction**\n",
    "\n",
    "<p style=\"text-align: justify;\">I am a data scientist. And my company wants me to build and NLP pipeline where it wants me to categorize the tweets as racist/sexist or not. The problem that currently we are facing is that there are many people on twitter which is now X, where they do now have someone to talk to when they get suicidical taughts. They generally pen down their taughts online. So our company want to help these people someone. And first problem that we have to we do not have model which can predict this wethere the tweets are related to suicide or not.</p>\n",
    "\n",
    "## üßø **Objective**\n",
    " <p style=\"text-align: justify;\">The objective of this project is that I will have to build a module which will try to classify the category of tweets. For this my company has given me a dataset which is in csv format. It has two columns, tweet and the suicide. Suicide column has labels whether the tweets are suicidical or not. So I will be using these data in the csv file and building an NLP model. Firstly I will be going through the full csv dataset and trying to remove the unwanted data. And then using these data I will be building a model. I will also be doing several experiments in order to improve the performance of the model. I will check if our dataset is balanced and if there is a requirement to increase the dataset so that our model can learn better from the data. Then I will choose the best-performing model to build my final model with the best-suited parameters.\n",
    " </p>\n",
    "\n",
    "**About the data**  \n",
    "This dataset contains 2 columns. This data set is from Kaggle. You can [**click here**](https://www.kaggle.com/datasets/aunanya875/suicidal-tweet-detection-dataset) to view the dataset on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tableofcontent'></a>\n",
    "## **Table of Content**\n",
    "\n",
    "##### ‚¨á [**Importing Libraries**](#library)\n",
    "##### üìä [**Importing Dataset**](#dataset)\n",
    "##### üó∫Ô∏è [**Exploring the dataset**](#exploration)\n",
    "##### üí≠ [**My taughts on the dataset**](#thoughts)\n",
    "##### ‚öôÔ∏è [**Pre-processing the data**](#processing)\n",
    "##### üîß [**Feature Engineering**](#feature)\n",
    "##### üóÇÔ∏è [**Selecting the best model for our dataset**](#selection)\n",
    "##### ‚úÖ [**Assessing which gives us the best results**](#assess)\n",
    "##### üéõÔ∏è [**Hyper-Parameter Tuning**](#tuning)\n",
    "##### üèÅ [**Final Model**](#Final)\n",
    "##### üí° [**Conclusion**](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Let's begin....***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='library'></a>\n",
    "## ‚¨á <span style=\"color: #20479b; font-weight: bold;\">Importing Libraries</span> <a href=\"#jason\" style=\"float: right; text-decoration: none; font-size: 24px;\">‚¨ÜÔ∏è</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing the necessary libraries for my Exploratory Data Analysis tasks in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import  pandas as  pd\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.model_selection\n",
    "from  imblearn.over_sampling import SMOTE\n",
    "from  imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,classification_report,confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import optuna\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>\n",
    "## üìä <span style=\"color: #20479b; font-weight: bold;\">Importing Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst_dir = '/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/016. sentiment_analysis_NLP_project/dataset/Suicide_Ideation_Dataset(Twitter-based).csv'\n",
    "dtst = pd.read_csv(dtst_dir,nrows=10000)\n",
    "print(display(dtst.sample(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## üó∫Ô∏è <span style=\"color: #20479b; font-weight: bold;\">Exploring the dataset</span> <a href=\"#toc\" style=\"float: right; text-decoration: none; font-size: 24px;\">‚¨ÜÔ∏è</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## üó∫Ô∏è <span style=\"color: #20479b; font-weight: bold;\">Exploring the dataset</span> <a href=\"#toc\" style=\"float: right; text-decoration: none; font-size: 24px;\">‚¨ÜÔ∏è</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that \n",
    "1. ID column is not necessary for us, hence we can delete it.\n",
    "2. tweets have alot of unnessary special characters, which we have to clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1787 rows of data which is good so that the model can learn better from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentage = {}\n",
    "for column in dtst:\n",
    "    null_percentage[column] = round(( ( dtst[column].isnull().sum() ) / len(dtst)) * 100,2)\n",
    "np_dataset = pd.DataFrame( list( null_percentage.items() ),  columns=[  'column_name'  , 'null_percentage' ])\n",
    "np_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are null entries in our dataset in tweet column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst[\"Suicide\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no unnessary entries too. Which is a good thing for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_ct = dtst['Suicide'].value_counts()\n",
    "print('Each class count:')\n",
    "sns.barplot(x=t_ct.index, y=t_ct.values)\n",
    "t_ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our dataset is ** imbalanced**. We need to balance the dataset so that we can have good model trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='thoughts'></a>\n",
    "## üí≠ <span style=\"color: #20479b; font-weight: bold;\">My thoughts on the dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per my initial Data Exploration,\n",
    "- There are null values.\n",
    "- I can see that the dataset big enough to build a good ML model.\n",
    "- I need to rename the suicide column to 0 and 1 \n",
    "- I will be also restructuring the column and moving the labels to the end to ease of reading.\n",
    "- The dataset consists of 31962 rows of data.\n",
    "- There are usertags and special characters in the column tweet. We need to remove those.\n",
    "- The dataset is heavily imbalanced. Hence we need to oversample the data.\n",
    "\n",
    "\n",
    "Evaluation Metrics:\n",
    "For this dataset I will be using, Accuracy, precision, F1 are the evaluation metrics.\n",
    "\n",
    "Considering all these above points, I will be following the below steps to \n",
    "\n",
    "**‚öôÔ∏è pre-process**\n",
    "1. üóëÔ∏è Delete the column ID.\n",
    "2. ü´∏üèª Move the column label to the end.\n",
    "3. üîÑ Convert the body to lower case\n",
    "4. üßπ Remove usertags.\n",
    "5. üßπ Remove special characters.\n",
    "<!-- 4. ‚ûó Spliting our main datset into two parts, one part for training and another for testing dataframe\n",
    "    - Doing it before testing or \n",
    "    - to avoid data Scalling and oversampling. -->\n",
    "\n",
    "**üîß feature Engineer**\n",
    "<!-- 1. ‚öñÔ∏è Scalling the numerical and categorical labels -->\n",
    "\n",
    "<!-- 2. üìà Oversampling the data for stroke = 1, Because of two main reasons.\n",
    "    - Our focus is on stroke detection and data for people who have stroke is very less.\n",
    "    - We do not have to worry about data size as well here, since it's not very huge. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='processing' ></a>\n",
    "## ‚öôÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Pre-processing the data</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. üóëÔ∏è Delete the null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst = dtst.dropna()\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Change the column values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['Suicide'] = dtst['Suicide'].str.lower()\n",
    "dtst['Suicide'] = dtst['Suicide'].str.replace(' ', '', regex=False)\n",
    "def chg_col_vl(vl):\n",
    "    if vl == 'notsuicidepost':\n",
    "        return 0\n",
    "    elif vl == 'potentialsuicidepost':\n",
    "        return 1\n",
    "    \n",
    "dtst['Suicide'] =  dtst['Suicide'].apply(chg_col_vl)\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. üîÑ Convert the body to lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst['Tweet'].str.lower()\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. #Ô∏è‚É£ Remove hashtags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst[\n",
    "                             'tweet_processed'\n",
    "                             ].replace(\n",
    "                                        r'#([^\\s]+)', r'\\1', regex=True\n",
    "                                        )\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.  üßπ Remove usertags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst['tweet_processed'].replace(r'@', '', regex=True)\n",
    "dtst.head(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.üóëÔ∏è Remove Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punc(tweet):\n",
    "    tweet = tweet.translate(str.maketrans('','',string.punctuation))\n",
    "    return tweet\n",
    "\n",
    "dtst['tweet_processed'] = dtst['tweet_processed'].apply(rem_punc)\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. üî¢ Clear all numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst[\n",
    "                            'tweet_processed'\n",
    "                            ].replace(\n",
    "                                        r'\\d+', '', regex=True\n",
    "                                        )\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. üßπ Remove special characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst['tweet_processed'] = dtst['tweet_processed'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "dtst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Remove unwanted giberish characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_gib_space(twt):\n",
    "    twt = twt.encode('ascii', 'ignore').decode('ascii')\n",
    "    twt = re.sub(r'\\s+', ' ', twt).strip()\n",
    "    return twt\n",
    "\n",
    "dtst['tweet_processed'] = dtst['tweet_processed'].apply(rem_gib_space)\n",
    "dtst.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessor(text):\n",
    "#     text = word_tokenize(text.lower())\n",
    "#     text = [PorterStemmer().stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "#     text = ' '.join(text)\n",
    "#     return text\n",
    "\n",
    "# dtst['tweet_processed'] = dtst['tweet_processed'].apply(preprocessor)\n",
    "\n",
    "dtst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# porter = nltk.SnowballStemmer('english')\n",
    "\n",
    "# def preprocessor(input_text):\n",
    "#     tokenized_text = nltk.word_tokenize(input_text, language='english')\n",
    "#     stems = [porter.stem(t) for t in tokenized_text]\n",
    "#     return ' '.join(stems)\n",
    "\n",
    "# dtst['tweet_processed'] = dtst['tweet_processed'].apply(preprocessor)\n",
    "\n",
    "# dtst.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Creating a TD-IDF matrix for the tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtst['tweet_processed'] = dtst['tweet_processed'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trm_mdl = TfidfVectorizer(min_df=2)\n",
    "m3 = trm_mdl.fit_transform(dtst['tweet_processed'])\n",
    "inf = pd.DataFrame(m3.toarray(), index=dtst.index)\n",
    "dtst = dtst.join(inf)\n",
    "inverted_index = {id: term for term, id in trm_mdl.vocabulary_.items()}\n",
    "dtst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are unwanted characters in my tweets data. How can I remove those? Here is some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. ‚ùé Splitting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = dtst['Suicide'].values\n",
    "tr = tr.astype(float)\n",
    "\n",
    "ft = dtst.drop(columns=[\n",
    "                        'Tweet',\n",
    "                        'tweet_processed',\n",
    "                        'Suicide'\n",
    "                        ])\n",
    "ft\n",
    "# ft = vectorizer.fit_transform(dtst['tweet_processed']).toarray()\n",
    "# ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'No of features     : {ft.shape[1]}')\n",
    "print(f'Total rows of data : {ft.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_lst = []\n",
    "# for itm in ft.columns:\n",
    "#     if itm in inverted_index:\n",
    "#         ft_lst.append(inverted_index[itm])\n",
    "#     else:\n",
    "#         ft_lst.append(itm)\n",
    "\n",
    "# ft = ft.values\n",
    "# ft = ft.astype('float')\n",
    "# ft_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train_df,  ft_val_test_df,   tr_train_df,  tar_val_test_df = sklearn.model_selection.train_test_split(  ft , tr  ,  test_size = 0.35, shuffle=True, stratify=tr, random_state  =  40)\n",
    "ft_val_df,  ft_test_df,   tar_val_df,  tar_test_df = sklearn.model_selection.train_test_split(  ft_val_test_df , tar_val_test_df  ,  test_size = 0.50,shuffle=True, stratify=tar_val_test_df,random_state  =  40)\n",
    "\n",
    "print(f'Train features shape     : {ft_train_df.shape}')\n",
    "print(f'Train target shape       : {tr_train_df.shape}')\n",
    "print(39*'=')\n",
    "print(f'Validation features shape: {ft_val_df.shape}')\n",
    "print(f'Validation target shape  : {tar_val_df.shape}')\n",
    "print(39*'=')\n",
    "print(f'Test features shape      : {ft_test_df.shape}')\n",
    "print(f'Test target shape        : {tar_test_df.shape}')\n",
    "print(39*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature' ></a>\n",
    "## üîß <span style=\"color: #20479b; font-weight: bold;\">Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. üìà Oversampling the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **SMOTE** oversampling to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_smt =  SMOTE(  )\n",
    "ft_train_df_smt, tr_train_df_smt   =    mod_smt.fit_resample(\n",
    "                                                            ft_train_df, \n",
    "                                                            tr_train_df)\n",
    "\n",
    "print(\"fet_train_smot size :\",ft_train_df_smt.shape)\n",
    "print(\"tar_train_smot size :\",tr_train_df_smt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **ADASYN** oversampling to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_adc =  ADASYN(  )\n",
    "ft_train_df_adc, tr_train_df_adc   =    mod_adc.fit_resample(\n",
    "                                                            ft_train_df, \n",
    "                                                            tr_train_df)\n",
    "\n",
    "print(\"fet_train_smot size :\",ft_train_df_adc.shape)\n",
    "print(\"tar_train_smot size :\",tr_train_df_adc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='selection'></a>\n",
    "## üóÇÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Selecting the best model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = sklearn.linear_model.LogisticRegression()\n",
    "svc = sklearn.svm.SVC()\n",
    "ran_for_cls =  sklearn.ensemble.RandomForestClassifier()\n",
    "xg_bst = XGBClassifier()\n",
    "# k_n_n =   sklearn.neighbors.KNeighborsClassifier()\n",
    "# gb = sklearn.ensemble.GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(ml, trf, trl, vlf, vll, mtd):\n",
    "        ml.fit(trf, trl)\n",
    "        p = pd.Series(ml.predict(vlf))\n",
    "        return [\n",
    "            mtd,\n",
    "            ml.__class__.__name__,\n",
    "            geometric_mean_score(vll, p),\n",
    "            f1_score(vll, p),\n",
    "            precision_score(vll, p),\n",
    "            recall_score(vll, p),\n",
    "            accuracy_score(vll, p)\n",
    "                ]\n",
    "\n",
    "mt_vl_lts = []\n",
    "lt = [log_reg,svc,ran_for_cls,xg_bst]\n",
    "cmb = [\n",
    "        ('IM', ft_train_df, tr_train_df),\n",
    "        ('SM', ft_train_df_smt, tr_train_df_smt),\n",
    "        ('AD', ft_train_df_adc, tr_train_df_adc)\n",
    "        ]\n",
    "for ml in lt:\n",
    "    for mtd, trf, trl in cmb:\n",
    "        mt_vl_lt = evaluate_model(ml, trf, trl, ft_val_df, tar_val_df, mtd)\n",
    "        if mt_vl_lt:\n",
    "            mt_vl_lts.append(mt_vl_lt)\n",
    "mt_vl_lts = pd.DataFrame(mt_vl_lts, columns=['Method', 'Model Name', 'G-mean','F1','PRE','REC', 'ACC'])\n",
    "mt_vl_lts = mt_vl_lts.sort_values(ascending=False, by=['G-mean','F1','PRE','REC', 'ACC'])\n",
    "\n",
    "display(mt_vl_lts.head(30))\n",
    "\n",
    "print('\\n')\n",
    "print('*****'*15)\n",
    "bm_name = mt_vl_lts.head(1)['Model Name'].values[0]\n",
    "osty = mt_vl_lts.head(1)['Method'].values[0]\n",
    "print(f'The best performing model is  :{bm_name} with oversampling method : {osty}')\n",
    "print('*****'*15)\n",
    "\n",
    "model_mapping = {\n",
    "    'LogisticRegression': LogisticRegression,\n",
    "    'SVC': SVC,\n",
    "    'RandomForestClassifier': RandomForestClassifier,\n",
    "    'XGBClassifier': XGBClassifier\n",
    "}\n",
    "\n",
    "# Retrieve the best model class\n",
    "bm = model_mapping[bm_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_dsets = {\n",
    "    'AD': {'X': ft_train_df_adc, 'y': tr_train_df_adc},\n",
    "    'SM': {'X': ft_train_df_smt, 'y': tr_train_df_smt},\n",
    "    'IM': {'X': ft_train_df, 'y': tr_train_df},\n",
    "}\n",
    "ft = tuning_dsets[osty]['X']\n",
    "tr = tuning_dsets[osty]['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Hyper'></a>\n",
    "## üéõÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Hyperparameter Tuning</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyptune(t):\n",
    "    prs = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 80, 950),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 9),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0, step=0.1),\n",
    "        'booster': 'gbtree',\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0)\n",
    "            }\n",
    "\n",
    "    hyml = bm(**prs, use_label_encoder=False)\n",
    "    hyml.fit(ft, tr)\n",
    "    hyp = hyml.predict(ft_val_df)\n",
    "    gm  = geometric_mean_score(tar_val_df, hyp)\n",
    "    fs  = f1_score(tar_val_df, hyp)\n",
    "    pre = precision_score(tar_val_df, hyp)\n",
    "    rec = recall_score(tar_val_df, hyp)\n",
    "    return gm\n",
    "    \n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "lrn = optuna.create_study(direction='maximize')\n",
    "lrn.optimize(objective, n_trials=200)\n",
    "bt = lrn.best_trial.value\n",
    "bp = lrn.best_trial.params\n",
    "print(f'Top gmean score          : {bt}')\n",
    "print(f'Top performing parameters: {bp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Final'></a>\n",
    "## üèÅ <span style=\"color: #20479b; font-weight: bold;\">Final Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(print(bp))\n",
    "display(print(bm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = study.best_trial.params\n",
    "fim = bm(**bp, use_label_encoder=False)\n",
    "fim.fit(ft, tr)\n",
    "fip = fim.predict(ft_test_df)\n",
    "fgm  = geometric_mean_score(tar_test_df, fip)\n",
    "ffs  = f1_score(tar_test_df, fip)\n",
    "fpre = precision_score(tar_test_df, fip)\n",
    "frec = recall_score(tar_test_df, fip)\n",
    "\n",
    "print(f'G-Mean Score: {fgm}')\n",
    "print(f'F1 Score    : {ffs}')\n",
    "print(f'Precision   : {fpre}')\n",
    "print(f'Recall      : {frec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Evaluation'></a>\n",
    "## üí° <span style=\"color: #20479b; font-weight: bold;\"> Model Evaluation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(tar_test_df, fip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(tar_test_df, fip)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plot, we were able to get very good scored and out of the 6 images we were able to predict most of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## üí° <span style=\"color: #20479b; font-weight: bold;\">Conclusion</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://icons8.com/icon/68193/twitter\">Twitter</a> icon by <a target=\"_blank\" href=\"https://icons8.com\">Icons8</a>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
