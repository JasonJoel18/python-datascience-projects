{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 2em;\">\n",
    "  <img src=\"/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/016. sentiment_analysis_NLP_project/dataset/icons8-twitter-pastel/icons8-twitter-256.png\" alt=\"Your Image\" style=\"height: 2.5em; margin-right: 15px;\">\n",
    "  Twitter Post Classification\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëãüèº **Introduction**\n",
    "\n",
    "<p style=\"text-align: justify;\">I am a data scientist. And my company wants me to build and NLP pipeline where it can categorize the tweets into suicidical or non suicidical tweets. The problem that currently most of them is facing is there are alot of times when people tweet about what they are feeling or when they are having suicidical taughts. But most of the times no one is there to help them. Hence the idea to to build a model where it can detect if it's a tweet related to suicide and give any sorts of help that the person needs.</p>\n",
    "\n",
    "## üßø **Objective**\n",
    " <p style=\"text-align: justify;\">The objective of this project is that I will have to build a module which will try to classify the category of twitter reviews. For this my company has given me a dataset which is in csv format. It has three columns, id, tweet and the label. So I will be using these data in the csv file and building an NLP model ???????. Firstly I will be going through the full csv dataset and trying to remove the unwanted data if unsupported extensions. And then using these data I will be building a model. I will also be doing several experiments in order to improve the performance of the model. I will check if our dataset is balanced and if there is a requirement to increase the dataset so that our model can learn better from the data. Then I will choose the best-performing model to build my final model with the best-suited parameters.\n",
    " </p>\n",
    "\n",
    "**About the data**  \n",
    "This dataset contains 12 columns. This data set is from Kaggle. You can [**click here**](https://www.kaggle.com/datasets/mayurdalvi/twitter-sentiments-analysis-nlp) to view the dataset on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Content**\n",
    "\n",
    "##### ‚¨á [**Importing Libraries**](#library)\n",
    "##### üìä [**Importing Dataset**](#dataset)\n",
    "##### üó∫Ô∏è [**Exploring the dataset**](#exploration)\n",
    "##### üí≠ [**My taughts on the dataset**](#thoughts)\n",
    "##### ‚öôÔ∏è [**Pre-processing the data**](#processing)\n",
    "##### üîß [**Feature Engineering**](#feature)\n",
    "##### üóÇÔ∏è [**Selecting the best model for our dataset**](#selection)\n",
    "##### ‚úÖ [**Assessing which gives us the best results**](#assess)\n",
    "##### üéõÔ∏è [**Hyper-Parameter Tuning**](#tuning)\n",
    "##### üèÅ [**Final Model**](#Final)\n",
    "##### üí° [**Conclusion**](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Let's begin....***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='library'></a>\n",
    "## ‚¨á <span style=\"color: #20479b; font-weight: bold;\">Importing Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing the necessary libraries for my Exploratory Data Analysis tasks in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn.model_selection\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>  \n",
    "\n",
    "## üìä <span style=\"color: #20479b; font-weight: bold;\">Importing Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/016. sentiment_analysis_NLP_project/dataset/Twitter Sentiments.csv'\n",
    "dataset = pd.read_csv(dataset_dir, nrows=10000)\n",
    "print(display(dataset.sample(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## üó∫Ô∏è <span style=\"color: #20479b; font-weight: bold;\">Exploring the dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that \n",
    "1. ID column is not necessary for us, hence we can delete it.\n",
    "2. tweets have alot of unnessary special characters, which we have to clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 31962 rows of data which is good so that the model can learn better from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as we can see there are three columns i.e. id, label and tweet.\n",
    "As per the data authors, 1 here stands for rasist/sexist tweet and 0 stands for non racist comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null entries in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no unnessary entries too. Which is a good thing for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our dataset is **heavily imbalanced**. We need to balance the dataset so that we can have good model trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='thoughts'></a>\n",
    "## üí≠ <span style=\"color: #20479b; font-weight: bold;\">My thoughts on the dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per my initial Data Exploration,\n",
    "- I can see that the dataset big enough to build a good ML model.\n",
    "- I need to removed the ID column as we do not need it. \n",
    "- I will be also restructuring the column and moving the labels to the end to ease of reading.\n",
    "- The dataset consists of 31962 rows of data.\n",
    "- There are usertags and special characters in the column tweet. We need to remove those.\n",
    "- The dataset is heavily imbalanced. Hence we need to oversample the data.\n",
    "\n",
    "\n",
    "Evaluation Metrics:\n",
    "For this dataset I will be using, Accuracy, precision, F1 are the evaluation metrics.\n",
    "\n",
    "Considering all these above points, I will be following the below steps to \n",
    "\n",
    "**‚öôÔ∏è pre-process**\n",
    "1. üóëÔ∏è Delete the column ID.\n",
    "2. ü´∏üèª Move the column label to the end.\n",
    "3. üîÑ Convert the body to lower case\n",
    "4. üßπ Remove usertags.\n",
    "5. üßπ Remove special characters.\n",
    "<!-- 4. ‚ûó Spliting our main datset into two parts, one part for training and another for testing dataframe\n",
    "    - Doing it before testing or \n",
    "    - to avoid data Scalling and oversampling. -->\n",
    "\n",
    "**üîß feature Engineer**\n",
    "<!-- 1. ‚öñÔ∏è Scalling the numerical and categorical labels -->\n",
    "\n",
    "<!-- 2. üìà Oversampling the data for stroke = 1, Because of two main reasons.\n",
    "    - Our focus is on stroke detection and data for people who have stroke is very less.\n",
    "    - We do not have to worry about data size as well here, since it's not very huge. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='processing' ></a>\n",
    "## ‚öôÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Pre-processing the data</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. üóëÔ∏è Delete the column ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop('id', axis=1)\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. ü´∏üèª Move the column label to the end**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = ['tweet', 'label']\n",
    "dataset = dataset[new_col_order]\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. üîÑ Convert the body to lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweet_processed'] = dataset['tweet'].str.lower()\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. #Ô∏è‚É£ Remove hashtags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweet_processed'] = dataset['tweet_processed'].replace(r'#([^\\s]+)', r'\\1', regex=True)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.  üßπ Remove usertags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweet_processed'] = dataset['tweet_processed'].replace(r'@[^\\s]+', '', regex=True)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.üóëÔ∏è Remove Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(tweet):\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    return tweet\n",
    "\n",
    "dataset['tweet_processed'] = dataset['tweet_processed'].apply(remove_punctuation)\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. üî¢ Clear all numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweet_processed'] = dataset['tweet_processed'].replace(r'\\d+', '', regex=True)\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. üßπ Remove special characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tweet_processed'] = dataset['tweet_processed'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Remove unwanted giberish characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gibberish_and_space(tweet):\n",
    "    tweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet\n",
    "\n",
    "dataset['tweet_processed'] = dataset['tweet_processed'].apply(remove_gibberish_and_space)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# porter = nltk.SnowballStemmer('english')\n",
    "\n",
    "# def preprocessor(input_text):\n",
    "#     tokenized_text = nltk.word_tokenize(input_text, language='english')\n",
    "#     stems = [porter.stem(t) for t in tokenized_text]\n",
    "#     return ' '.join(stems)\n",
    "\n",
    "# dtst['tweet_processed'] = dtst['tweet_processed'].apply(preprocessor)\n",
    "\n",
    "# dtst.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Creating a TD-IDF matrix for the tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtst['tweet_processed'] = dtst['tweet_processed'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "dataset = dataset.join(pd.DataFrame(vectorizer.fit_transform(dataset['tweet_processed']).toarray()))\n",
    "inverted_index = {id: term for term, id in vectorizer.vocabulary_.items()}\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are unwanted characters in my tweets data. How can I remove those? Here is some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = '#model   i love u take with u all the time in ur√∞¬ü¬ì¬±!!! √∞¬ü¬ò¬ô√∞¬ü¬ò¬é√∞¬ü¬ë¬Ñ√∞¬ü¬ë¬Ö",
    "√∞¬ü¬í¬¶√∞¬ü¬í¬¶√∞¬ü¬í¬¶'\n",
    "# hi = text.encode('ascii', 'ignore').decode('ascii')\n",
    "# hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. ‚ùé Splitting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataset['label'].values.astype(float)\n",
    "\n",
    "features = dataset.drop(columns=['tweet', 'tweet_processed', 'label']).values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, val_test_features, train_target, val_test_target = sklearn.model_selection.train_test_split(  features , target  ,  test_size = 0.35, shuffle=True, stratify=target, random_state  =  40)\n",
    "val_features,  test_features,   val_target,  test_target = sklearn.model_selection.train_test_split(  val_test_features , val_test_target  ,  test_size = 0.50,shuffle=True, stratify=val_test_target,random_state  =  40)\n",
    "\n",
    "print(f'Train features shape     : {train_features.shape}')\n",
    "print(f'Train target shape       : {train_target.shape}')\n",
    "print(39*'=')\n",
    "print(f'Validation features shape: {val_features.shape}')\n",
    "print(f'Validation target shape  : {val_target.shape}')\n",
    "print(39*'=')\n",
    "print(f'Test features shape      : {test_features.shape}')\n",
    "print(f'Test target shape        : {test_target.shape}')\n",
    "print(39*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature' ></a>\n",
    "## üîß <span style=\"color: #20479b; font-weight: bold;\">Feature Engineering</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. üìà Oversampling the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **SMOTE** oversampling to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **12. Balancing the data using SMOTE**\n",
    "# ```python\n",
    "print(f'Original Training Target Data Distribution: {np.bincount(train_target.astype(int))}')\n",
    "\n",
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=0)\n",
    "train_features, train_target = smote.fit_resample(train_features, train_target)\n",
    "\n",
    "print(f'Resampled Training Target Data Distribution: {np.bincount(train_target.astype(int))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **NGRAM** oversampling to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract n-grams\n",
    "def generate_ngrams(text, n=2):\n",
    "    words = text.split()\n",
    "    ngrams = zip(*[words[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "dataset['bigrams'] = dataset['tweet_processed'].apply(lambda x: generate_ngrams(x, 2))\n",
    "dataset['trigrams'] = dataset['tweet_processed'].apply(lambda x: generate_ngrams(x, 3))\n",
    "\n",
    "# Convert n-grams into separate columns in the dataframe\n",
    "bigrams_vectorizer = TfidfVectorizer(min_df=2)\n",
    "trigrams_vectorizer = TfidfVectorizer(min_df=2)\n",
    "\n",
    "bigrams_matrix = bigrams_vectorizer.fit_transform(dataset['bigrams'].apply(lambda x: \" \".join(x)))\n",
    "trigrams_matrix = trigrams_vectorizer.fit_transform(dataset['trigrams'].apply(lambda x: \" \".join(x)))\n",
    "\n",
    "# Convert sparse matrix to dataframe and concatenate with the original dataframe\n",
    "bigrams_df = pd.DataFrame(bigrams_matrix.toarray(), columns=[f'bigram_{i}' for i in range(bigrams_matrix.shape[1])])\n",
    "trigrams_df = pd.DataFrame(trigrams_matrix.toarray(), columns=[f'trigram_{i}' for i in range(trigrams_matrix.shape[1])])\n",
    "\n",
    "dataset = pd.concat([dataset, bigrams_df, trigrams_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SENTIMENT SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to calculate sentiment score\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "dataset['sentiment_score'] = dataset['tweet_processed'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(train_features, train_target)\n",
    "\n",
    "logistic_predictions = logistic_model.predict(val_features)\n",
    "\n",
    "print(f'Logistic Regression Accuracy: {accuracy_score(val_target, logistic_predictions)}')\n",
    "print(f'Logistic Regression F1 Score: {f1_score(val_target, logistic_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier()\n",
    "random_forest_model.fit(train_features, train_target)\n",
    "\n",
    "rf_predictions = random_forest_model.predict(val_features)\n",
    "\n",
    "print(f'Random Forest Accuracy: {accuracy_score(val_target, rf_predictions)}')\n",
    "print(f'Random Forest F1 Score: {f1_score(val_target, rf_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(train_features, train_target)\n",
    "\n",
    "gb_predictions = gb_model.predict(val_features)\n",
    "\n",
    "print(f'Gradient Boosting Accuracy: {accuracy_score(val_target, gb_predictions)}')\n",
    "print(f'Gradient Boosting F1 Score: {f1_score(val_target, gb_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feature' ></a>\n",
    "## üîß <span style=\"color: #20479b; font-weight: bold;\">Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_score(y_true, y_pred):\n",
    "    return geometric_mean_score(y_true, y_pred) * accuracy_score(y_true, y_pred) * f1_score(y_true, y_pred)\n",
    "\n",
    "def apply_model(clf_class, **kwargs):\n",
    "    clf = clf_class(**kwargs)\n",
    "    print(clf_class)\n",
    "    print(35 * '=')\n",
    "\n",
    "    clf.fit(ft_train_df, tr_train_df)\n",
    "    pred = clf.predict(ft_val_df)\n",
    "    gmean = geometric_mean_score(tar_val_df, pred)\n",
    "    acc = accuracy_score(tar_val_df, pred)\n",
    "    f1 = f1_score(tar_val_df, pred)\n",
    "    prod = product_score(tar_val_df, pred)\n",
    "    print('Imbalanced: G-Mean = {:.3f} Accuracy = {:.3f} f1 = {:.3f} Product = {:.3f}'.format(gmean, acc, f1, prod))\n",
    "\n",
    "    clf.fit(ft_train_df_smt, tr_train_df_smt)\n",
    "    pred = clf.predict(ft_val_df)\n",
    "    gmean = geometric_mean_score(tar_val_df, pred)\n",
    "    acc = accuracy_score(tar_val_df, pred)\n",
    "    f1 = f1_score(tar_val_df, pred)\n",
    "    prod = product_score(tar_val_df, pred)\n",
    "    print('SMOTE: G-Mean = {:.3f} Accuracy = {:.3f} f1 = {:.3f} Product = {:.3f}'.format(gmean, acc, f1, prod))\n",
    "\n",
    "    clf.fit(ft_train_df_adc, tr_train_df_adc)\n",
    "    pred = clf.predict(ft_val_df)\n",
    "    gmean = geometric_mean_score(tar_val_df, pred)\n",
    "    acc = accuracy_score(tar_val_df, pred)\n",
    "    f1 = f1_score(tar_val_df, pred)\n",
    "    prod = product_score(tar_val_df, pred)\n",
    "    print('ADASYN: G-Mean = {:.3f} Accuracy = {:.3f} f1 = {:.3f} Product = {:.3f}'.format(gmean, acc, f1, prod))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "def compare_models():\n",
    "    apply_model(GradientBoostingClassifier)\n",
    "    apply_model(RandomForestClassifier)\n",
    "    apply_model(LogisticRegression)\n",
    "\n",
    "compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='selection'></a>\n",
    "## üóÇÔ∏è <span style=\"color: #20479b; font-weight: bold;\">Experimental Section</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I have 10 different configurations and choose the best out of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  table {\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "    border: 1px solid #ddd;\n",
    "    font-family: Arial, sans-serif;\n",
    "  }\n",
    "  th, td {\n",
    "    padding: 12px;\n",
    "    text-align: left;\n",
    "    border-bottom: 1px solid #ddd;\n",
    "  }\n",
    "  th {\n",
    "    background-color: #03bafc;\n",
    "    color: #000;\n",
    "  }\n",
    "  tr:nth-child(even) {\n",
    "    background-color: #EEEEEE;\n",
    "  }\n",
    "  tr:hover {\n",
    "    background-color: #DBECFA; /* Light tint of blue for hover effect */\n",
    "  }\n",
    "  td:first-child {\n",
    "    font-weight: bold;\n",
    "  }\n",
    "   tr.mdl_config8 {\n",
    "    background-color: #95CD97; /* Dark light green */\n",
    "  } \n",
    "  .mdl_config8 {\n",
    "    background-color: #95CD97; /* Darker green for note */\n",
    "    padding: 8px;\n",
    "    font-style: italic;\n",
    "\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model Config</th>\n",
    "      <th>Layers</th>\n",
    "      <th>Validation Accuracy</th>\n",
    "      <th>Observations</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><b>mdl_config1</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.8375</td>\n",
    "      <td>The accuracy was very low and it was fluctuating a lot. Hence, I will add one more dense hidden layer to the model.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config2</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(16, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.4312</td>\n",
    "      <td>With the additional layer with 16 neurons, the accuracy dipped to 43%. I wanted to check if making the network more complex would help the model predict better.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config3</b></td>\n",
    "      <td>Flatten(input_shape=(256,256,3))<br>Dense(16, activation='relu')<br>tf.keras.layers.Dense(32, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.63125</td>\n",
    "      <td>With the addition of another layer for 16 neurons, the accuracy increased by approximaterly 20%. Since these are images I wanted to check addition of convolutional and maxpooling layer will help, I will also increase the neurons for the dense layer</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config4</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.93125</td>\n",
    "      <td>Due to the convlutional layers it was able to read the images better. It was able to extract various features from the dataset. The accuracy has not increased to 93. Now I want to check it adding one more layer will help increase the accuracy.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config5</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.93750</td>\n",
    "      <td>There was no significant increace in the accuracy after adding another convolutional layer. But it was still slightly better than the previous configuration. Just to experiment I want to add another layer of convolution with more neurons and see if it increases the performance.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config6</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')</td>\n",
    "      <td>0.99375</td>\n",
    "      <td>This was a huge improvement for me. The accuracy went to 99. The accuracy is fluctuating a little bit though. I do not want the model to overfit the train dataset hence I will experiment adding Early Stopping and see how it impacts</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config7</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.996324</td>\n",
    "      <td>The accuracy has improved slightly and the accuracy curve is also much better. To avoid further overfit I will add Dropout layer see how it impacts</td>\n",
    "    <tr class=\"mdl_config8\">\n",
    "      <td><b>üëë mdl_config8</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td class=\"observations\">The accuracy has decreased slightly, this may be because it was overfitting the data earlier. Let me add batch normalization and check if it will improve the accuracy.\n",
    "      <br><br><b>Note:</b> I have chosen this even if model 7 has better results because this model has both Early Stopping and Dropout, and even with that, it was able to score 0.98750 without overfitting the train data, which is great!!</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config9</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>BatchNormalization()<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td>The accuracy is still the same. Now I want to check if changing the optimizers to sgd from adam will help increase the acccuracy</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config10</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>BatchNormalization()<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td>With sgd the accuracy dropped to 72, and the accuracy fluctuates alot. So I will be going back to the adam optimizer. And I will try increasing the learning rate</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>mdl_config11</b></td>\n",
    "      <td>Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(32, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Conv2D(64, (2,2), 1, activation='relu', input_shape=(256,256,3))<br>MaxPooling2D()<br>Flatten()<br>Dense(256, activation='relu')<br>BatchNormalization()<br>Dropout(0.5)<br>Dense(4, activation='softmax')<br>++EarlyStopping</td>\n",
    "      <td>0.98750</td>\n",
    "      <td>With increase in learning rate thethe accuracy fluctuates even more.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Final'></a>\n",
    "## üèÅ <span style=\"color: #20479b; font-weight: bold;\">Final Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_config8 = Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (2,2), 1, activation='relu', input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), 1, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "mdl_config8.compile('adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='validation_loss',mode='max',patience=4, restore_best_weights=True)\n",
    "hist_config8 = mdl_config8.fit(train_btc_dt, epochs=20, validation_data=validation_btc_dt, callbacks=[early_stop])\n",
    "print('')\n",
    "print('=====================================================================================================')\n",
    "print('')\n",
    "\n",
    "mdl_config8.summary()\n",
    "\n",
    "fig, (acc, lss) = plt.subplots(1, 2, figsize=(7.5, 3))\n",
    "\n",
    "acc.plot(hist_config8.history['accuracy'], color='red', label='accuracy')\n",
    "acc.plot(hist_config8.history['val_accuracy'], color='blue', label='val_accuracy')\n",
    "acc.set_title('Accuracy', fontsize=20)\n",
    "acc.legend(loc=\"upper left\")\n",
    "\n",
    "lss.plot(hist_config8.history['loss'], color='red', label='train_loss')\n",
    "lss.plot(hist_config8.history['val_loss'], color='blue', label='validation_loss')\n",
    "lss.set_title('Loss', fontsize=20)\n",
    "lss.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Evaluation'></a>\n",
    "## üí° <span style=\"color: #20479b; font-weight: bold;\"> Model Evaluation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "recl_lst = []\n",
    "prcn_lst = []\n",
    "acry_lst = []\n",
    "\n",
    "imgs = []\n",
    "tarpreds = []\n",
    "tars = []\n",
    "\n",
    "for btc in test_btc_dt.as_numpy_iterator():\n",
    "    img, tar = btc\n",
    "    tarpred = mdl_config8.predict(img)\n",
    "    tarpred = tarpred.argmax(axis=1)\n",
    "    recln = recall_score(tar, tarpred, average='macro')\n",
    "    prcnn = precision_score(tar, tarpred, average='macro')\n",
    "    acryn = accuracy_score(tar, tarpred)\n",
    "    prcn_lst.append(prcnn)\n",
    "    recl_lst.append(recln)\n",
    "    acry_lst.append(acryn)\n",
    "\n",
    "    # Append images, predictions, and ground truths for visualization\n",
    "    imgs.extend(img)\n",
    "    tarpreds.extend(tarpred)\n",
    "    tars.extend(tar)\n",
    "\n",
    "    # Break the loop if enough images are collected\n",
    "    if len(imgs) >= 6:\n",
    "        break\n",
    "\n",
    "print(f'Precision : {sum(prcn_lst)/len(prcn_lst)}')\n",
    "print(f'Recall    : {sum(recl_lst)/len(recl_lst)}')\n",
    "print(f'Accuracy  : {sum(acry_lst)/len(acry_lst)}')\n",
    "\n",
    "fig, am = plt.subplots(2, 3, figsize=(12, 8))\n",
    "am = am.ravel()\n",
    "\n",
    "for i,img in enumerate(imgs):\n",
    "    # Plot image\n",
    "    am[i].imshow(img)\n",
    "    am[i].set_xticks([])\n",
    "    am[i].set_yticks([])\n",
    "    \n",
    "    if tarpreds[i] == tar[i]:\n",
    "        ttl = 'green'\n",
    "    else:\n",
    "        ttl = 'red'\n",
    "    \n",
    "    am[i].set_title(f'Predicted: {eq_class[tarpreds[i]]}\\nActual: {eq_class[tars[i]]}', color=ttl)\n",
    "\n",
    "    if i >=5:\n",
    "        break\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above plot, we were able to get very good scored and out of the 6 images we were able to predict most of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## üí° <span style=\"color: #20479b; font-weight: bold;\">Conclusion</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://icons8.com/icon/68193/twitter\">Twitter</a> icon by <a target=\"_blank\" href=\"https://icons8.com\">Icons8</a>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
