{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "We will build a machine learning pipeline using a linear regression model. In particular, you should do the following:\n",
    "Steps:-\n",
    "- Load the `canada_per_capita_income` dataset using [Pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). You can find this dataset in the datasets folder.\n",
    "- Split the dataset into training and test sets using [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). \n",
    "- Conduct data exploration, data preprocessing, and feature engineering if necessary. \n",
    "- Train and test a linear regression model using [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "- Check the documentation to identify the most important hyperparameters, attributes, and methods of the model. Use them in practice.\n",
    "- Give a Conclusion to the of the study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing the necessary libraries for my data analysis and machine learning tasks in Python. The `pandas` library helps me handle data efficiently, so I import it as `pd`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.metrics import r2_score\n",
    "# This is a test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data for Per Capita Income Prediction\n",
    "\n",
    "Then, I read a CSV file containing data on Canada's per capita income over the years.\\\n",
    "- The file path is specified as `.../canada_per_capita_income.csv`.\n",
    "- Using `pd.read_csv()`, I load the data from the CSV file into a pandas DataFrame named `df`.\\\n",
    "- This DataFrame serves as the primary data structure for my analysis.\n",
    "- Once the data is loaded, I can explore its structure, characteristics, and start preparing it for further analysis or machine learning model development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/008. predicting_per_capita_income_based_on_year/dataset/canada_per_capita_income.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Data Dimensions\n",
    "\n",
    "After loading the dataset into the DataFrame `df`, I'm interested in understanding its dimensions, which helps me grasp the size of the dataset.\n",
    "\n",
    "Using the `.shape` attribute of the DataFrame, I retrieve a tuple containing two values: the number of rows and the number of columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data for Training and Testing\n",
    "\n",
    "Now we have reached a crucial step: splitting the data into training and testing sets. This ensures that I can train my model on one subset of the data and evaluate its performance on another, unseen subset.\n",
    "\n",
    "Using the `train_test_split` function from the `sklearn.model_selection` module, I split my dataset into training and testing sets. Specifically, I split the features (`df['year']`) and the target variable (`df['per capita income (US$)']`) into training and testing sets, with a test size of 0.25 (25%).\n",
    "\n",
    "The resulting sets are:\n",
    "\n",
    "- `X_train`: The training features, representing years.\n",
    "- `X_test`: The testing features, also representing years.\n",
    "- `y_train`: The target variable for training, representing per capita income (US$).\n",
    "- `y_test`: The target variable for testing, also representing per capita income (US$).\n",
    "\n",
    "After the split, I inspect the shapes of these sets to ensure they align correctly. The shapes are as follows:\n",
    "\n",
    "- Shape of X_train: (shape of the training feature set)\n",
    "- Shape of X_test: (shape of the testing feature set)\n",
    "- Shape of y_train: (shape of the training target set)\n",
    "- Shape of y_test: (shape of the testing target set)\n",
    "\n",
    "These shapes provide insights into the distribution of data between training and testing sets, ensuring that my model receives a balanced and representative subset for both training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(df['year'],df['per capita income (US$)'], test_size=0.25)\n",
    "\n",
    "print(f\"Shape of X_train is: {X_train.shape}\")\n",
    "print(f\"Shape of X_test is: {X_test.shape}\")\n",
    "print(f\"Shape of y_train is: {y_train.shape}\")\n",
    "print(f\"Shape of y_train is: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_train, x=\"bedrooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_train:\n",
    "    new = df_train[column].value_counts()\n",
    "    print(new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_train:\n",
    "    new = df_train[column].unique()\n",
    "    print(f\"Column Name : {column}\")\n",
    "    print(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.drop(['price'], axis=1)\n",
    "y_train = df_train['price']\n",
    "x_test = df_test.drop(['price'], axis=1)\n",
    "y_test = df_test['price']\n",
    "\n",
    "print(f\"X Train size: {x_train.shape}\")\n",
    "print(f\"Y Train size: {y_train.shape}\")\n",
    "print(f\"X Test size: {x_test.shape}\")\n",
    "print(f\"Y Train size: {y_test.shape}\")\n",
    "\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_attributes = x_train.select_dtypes(include=[\"int64\",\"float64\"])\n",
    "numerical_attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "scaler.transform(x_train)\n",
    "scaler.transform(x_test)\n",
    "\n",
    "print(f\"X Train size: {x_train.shape}\")\n",
    "print(f\"X Test size: {x_test.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.svm.SVC()\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vectors = model.support_vectors_\n",
    "gram_matrix = np.dot(support_vectors, support_vectors.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(x_test)\n",
    "MSE = mean_squared_error(y_test, y_predicted)\n",
    "MAE = mean_absolute_error(y_test, y_predicted)\n",
    "R2 = r2_score(y_test, y_predicted)\n",
    "\n",
    "print(f\" MSE : {MSE}\")\n",
    "print(f\" MAE : {MAE}\")\n",
    "print(f\" R2  : {R2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
