{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1 style=\"text-align: center;\">üïµüèª Stroke Prediction Project üèéÔ∏è</h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëãüèº ***Introduction***\n",
    "\n",
    "<p style=\"text-align: justify;\">I am a data scientist for a company called TrackFit. And it is building an application which tracks the health data for the user. And they are planning to add an additonal function where it will take some inputs and track the bmi and based on these features it will try to predict whether user has chances of getting a stroke or not. Strokes are one of the leading cause of deaths all round the world. The aim of our company is reduce the number of deaths that are caused due to heart stroke. But giving this stroke prediction function to the user, we are make the user beware of the potential harm and inform him/her to get a doctors opinion at the earliest, which will result is reducing the cause of deaths.</p>\n",
    "\n",
    "\n",
    "## üßø ***Objective***\n",
    " <p style=\"text-align: justify;\">Then objective of this project is to build an machine  learning module which will predict whether an user has chances of getting a hear stroke. For this my company are given me a dataset which has many metrics which has impact on the cause of stroke. So I will be using these features and building an accurate machine learning model. I have specified the steps we will be following in the **Table of content** so that it will be easier to navigate.\n",
    " \n",
    " I have considered this as a machine learning task as we will in involved the likelyhood of an indivisual having stroke or not based on the data sample. We will be deviding this data into two parts.  \n",
    " </p>\n",
    "\n",
    "**About the data**  \n",
    "This [**dataset**](https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data/code) contains 16 columns of sales data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Table of Content***\n",
    "\n",
    "##### ‚¨á [**Importing Libraries**](#library)\n",
    "##### üìä [**Importing Dataset**](#dataset)\n",
    "##### üó∫Ô∏è [**Exploring the dataset**](#exploration)\n",
    "##### üí≠ [**My taughts on the dataset**](#thoughts)\n",
    "##### ‚öôÔ∏è [**Pre-processing the data**](#processing)\n",
    "##### ü§ñ [**Feature Engineering**](#processing)\n",
    "##### ü§ñ [**Model Training**](#processing)\n",
    "##### ‚úÖ [**Model Assessment**](#processing)\n",
    "##### üí° **Conclusion**  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Let's go....***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='library'></a>\n",
    "## ‚¨á <span style=\"color: #20479b; font-weight: bold;\">Importing Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by importing the necessary libraries for my Exploratory Data Analysis tasks in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "from dateutil import parser\n",
    "import datetime as dt\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patheffects as path_effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>  \n",
    "\n",
    "## üìä <span style=\"color: #20479b; font-weight: bold;\">Importing Dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jasonjoelpinto/Documents/GitHub/python-datascience-projects/010. stroke_prediction/dataset/healthcare-dataset-stroke-data.csv')\n",
    "\n",
    "print(\"No. of rows.   :\", df.shape[0])\n",
    "print(\"No. of cols.   :\", df.shape[1])  \n",
    "print(\"=\"*30)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting `Test` and `Train` data. Because we do not want data leakage here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['stroke'],axis=1)\n",
    "y = df['stroke']\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x,y)\n",
    "\n",
    "print(\"df size:\",df.shape)\n",
    "print(\"df train size:\",x_train.shape)\n",
    "print(\"df test size:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## üó∫Ô∏è <span style=\"color: #20479b; font-weight: bold;\">Exploring the dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "For EDA and Data Pre-processing I will be using the main datafram. Once dataframe is cleaned. I will be splitting it into `train` and  `test` split. Here's why\n",
    "1. I do not want the just the `test` data to be cleaned and `train` data having unwanted data and error.\n",
    "2. If I clean only the `test` data then my data is not generalized. Hence while my model tries to predict it will have different issues during training and testing.\n",
    "\n",
    "After loading the dataset into the DataFrame `df`, I will check the `df` shape using the `.shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above there are 5110 rows and 12 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use `.info` method to get a shortsummary of the DataFrame. It will include the number of non-null values, Data Types, and Memory Usage etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that the \n",
    "- count in the **Non-Null** column is less than the total rows for columns `bmi` i.e. 5110\n",
    "- that means that there are many NULL values in the dataset. \n",
    "- which we will take care in the data cleaning part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use `dtypes` attribute to get the datatypes of the columns.  \n",
    "I will also get the same details in `.info()` attribute above. But I like to view it seperately we get a getter understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use `df.describe()` \n",
    "- to generate a descriptive statistics of numerical columns of the DataFrame. \n",
    "- It will provide us various functions such as mean, count, Standard Deviation, Min, Max and percentiles.\n",
    "- I have used **Transpose** here as I want the columns of my DataFrame as the index.\n",
    "- This especially helps when I have many numrical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **age**:\n",
    "   - As we can see here we have data from the age of 0.8 to 82 years.\n",
    "   - Average age is near 43 years.\n",
    "   - MIN is 0.08, we will have to check why there is entry below 1.\n",
    "\n",
    "2. **hypertension**:\n",
    "   - The hypertension values ranges between 0 and 1. 0 means no hypertension 1 means person has hypertension.\n",
    "   - Average values of this colum in 0.09 which means that not many people have hypertension.\n",
    "\n",
    "3. **heart_disease**:\n",
    "   - Average values of this colum in 0.05 which means that not many people have heart disease.\n",
    "\n",
    "4. **avg_glucose_level**:\n",
    "   - The average glucose level of people in this data ranges between 55 to 271 mmol/L.\n",
    "   - Mean is 106 mmol/L.\n",
    "\n",
    "5. **bmi**:\n",
    "   - bmi is ranging from 10 to 97 bmi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will check now many null values are there in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows.   :\", df.shape[0])\n",
    "print(\"=\"*30)\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to check how much percentage of the columns are null. If they are negligible percentage then we can just drop the columns. Otherwise I will fill the na values with the meaninfull values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentage = {}\n",
    "for column in df:\n",
    "    null_percentage[column] = round(( ( df[column].isnull().sum() ) / len(df)) * 100,2)\n",
    "np_df = pd.DataFrame( list( null_percentage.items() ),  columns=[  'column_name'  , 'null_percentage' ])\n",
    "np_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the null percentage of column `bmi` is negligible when compared to the size of the dataset, we can drop these null rows as bmi cannot be `NULL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"gender\"].unique())\n",
    "print(df[\"work_type\"].unique())\n",
    "print(df[\"Residence_type\"].unique())\n",
    "print(df[\"smoking_status\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values look fine here, there are no unwanted/error values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.duplicated().any()` I will be checking if there are any duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to see data for all age groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check why there was data for age less than 1 year old's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['age']<1].shape)\n",
    "print('='*30)\n",
    "df[df['age']<1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 42 rows, and the bmi looks less. There are chances that these are records for the infants. Let's check if all the records `worktype` is children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['age']<1].equals(df[(df['age']<1) & (df['work_type'] == 'children')]))\n",
    "print('='*30)\n",
    "df.loc[df['age']<1,['bmi']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can both the dataframes match and also teh bmi ais less is very less. Hence we I think that records are for infants. So we need not delete these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (p1,p2,p3) = plt.subplots(1,3, figsize=(13, 6), dpi=300)\n",
    "\n",
    "propotion = df['hypertension'].value_counts()\n",
    "names = ['No','Yes']\n",
    "p1.pie(propotion, labels=names, autopct='%1.1f%%')\n",
    "p1.set_title('Hypertension', fontweight='bold')\n",
    "\n",
    "propotion = df['heart_disease'].value_counts()\n",
    "names = ['No','Yes']\n",
    "p2.pie(propotion, labels=names, autopct='%1.1f%%')\n",
    "p2.set_title('Heart Disease', fontweight='bold')\n",
    "\n",
    "propotion = df['ever_married'].value_counts()\n",
    "names = ['No','Yes']\n",
    "p3.pie(propotion, labels=names, autopct='%1.1f%%')\n",
    "p3.set_title('Married', fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this above plot you can see that\n",
    "1. **Hypertension:** Only 9.2% of all the people have Hypertension problem.\n",
    "2. **Heart Disease:** About 95.0% of all the people do not have Heart Disease problem.\n",
    "3. **Married:** About 34.7% are married.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), dpi=300)\n",
    "\n",
    "sns.histplot(data=df, x=\"avg_glucose_level\", ax=ax1)\n",
    "sns.histplot(data=df, x=\"bmi\", ax=ax2)\n",
    "plt.subplots_adjust(left=0.1, right=0.8, top=0.8, bottom=0.1, wspace=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_of_numerical_cols = df.groupby('stroke')[['bmi', 'avg_glucose_level', 'age']].mean().reset_index()\n",
    "names = ['No Stroke',  'Stroke']\n",
    "colors = ['#4CBB17',   'tab:red']\n",
    "barwidth = 0.4\n",
    "\n",
    "fig, p = plt.subplots(1, 3, figsize=(13, 6), dpi=400)\n",
    "\n",
    "for i, col in enumerate(['bmi', 'avg_glucose_level', 'age']):\n",
    "    x = avg_of_numerical_cols['stroke']\n",
    "    y = avg_of_numerical_cols[col]\n",
    "\n",
    "\n",
    "    p[i].bar(x,  y, label=names,  color =  colors, width =  barwidth, capstyle='round')\n",
    "    p[i].set_title(f'Average {col} for data\\n with stroke and without.', fontweight ='bold', pad  =20)\n",
    "    p[i].set_ylabel(col , fontweight='bold')\n",
    "    p[i].set_xticks(x)\n",
    "    p[i].set_xticklabels(names, fontweight='bold')\n",
    "    p[i].legend(title='Color'  )\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.0, right=2.4, top=1, bottom=0.1, wspace=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data we can see that\n",
    "1. **bmi:** The relationship between bmi and bmi and having stroke is very negligible.\n",
    "2. **Average Glucode Levels:** If a persons average glucose is more than 130, then he is very likely to have a stroke.\n",
    "3. **Average Glucode Levels:** Also we can see that there are significant chances of stroke among people aged on average of 65 n above. It is significantly less amond mid aged and below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type']\n",
    "\n",
    "plt.figure(figsize=(25, 18), dpi=400)\n",
    "\n",
    "for index, columns in enumerate(column_names):\n",
    "    plt.subplot(2, 3, index+1)\n",
    "    sns.countplot(y=columns, data=df, palette='Set2', hue='stroke')\n",
    "    plt.legend(labels=['No Stroke', 'Stroke'])\n",
    "    plt.title(f'Stroke Count by {columns}', fontweight='bold', pad=20)\n",
    "    plt.ylabel(columns, fontweight='bold')\n",
    "    plt.xlabel('Count', fontweight='bold')\n",
    "    # plt.gca().set_yticklabels(df['work_type'], rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.subplots_adjust(left=0.1, right=1, top=0.6, bottom=0.1, wspace=0.2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='thoughts'></a>\n",
    "## üí≠ <span style=\"color: #20479b; font-weight: bold;\">My thoughts on the dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per my initial Data Exploration,\n",
    "- I can see that the dataset big enough to build a ML model.\n",
    "- This dataset consits of null values in   `bmi` column. Which I can delete it as it negligible rows of data and the bmi cannot be `null`.\n",
    "- I will be droping column `id` as we do not need identity column while building a prediction model as will bring the accuracy down.\n",
    "- Other than that the data looks pretty clean and I may not have to do a lot of data cleaning.\n",
    "- As I will have to train the model and then later test it, I will be splitting the data into two parts. Train and Test split.\n",
    "\n",
    "\n",
    "\n",
    "I will be following the below steps to üßπ clean my data for further analysis.\n",
    "1. ‚ùé Drop all the rows containing `null` values.\n",
    "2. ‚ùé Drop columns `id`.\n",
    "3. ‚úÖ Re-arrange the columns for better view. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='processing' ></a>\n",
    "## üßπ <span style=\"color: #20479b; font-weight: bold;\">Pre-processing the data</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the null_percentage for each column is negligible. The max is for Transmission is approximaterly 3%, which is negligible when we consider the datasize. Hence I will be droping the rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows.   :\", df.shape[0])\n",
    "print(\"=\"*30)\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`id` column is not usefull for my analysis, hence I will be droping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"id\",axis=1,inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x=df['stroke'])\n",
    "plt.title('Distribution of Target Label')\n",
    "plt.xlabel('Target Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will replace the make names with standard one's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
